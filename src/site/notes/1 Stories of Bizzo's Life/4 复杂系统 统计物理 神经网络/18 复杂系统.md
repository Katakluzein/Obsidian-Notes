---
{"dg-publish":true,"permalink":"/1 Stories of Bizzo's Life/4 复杂系统 统计物理 神经网络/18 复杂系统/","tags":["热力学与统计物理"]}
---


> [!quote] 
> When you think about the complexity of our natural world - plants using quantum mechanics for photosynthesis, for example - a smartphone begins to look like a pretty dumb object. 
>  ——Jeff Vandermeer 

> [!quote] 

> [!abstract] Contents
> - 时间之箭
> - 动力学系统
> - 分形
> - 自组织临界
> - 神经网络

## 复杂系统与统计物理简介
如果可以把一个复杂系统比作一个生命有机体，那么科学家的使命就是去感受其生命的臆动，倾听其生命的呼吸，探索其生命的规律[[Zhengzhigang\|Zhengzhigang]]。\par 
系统科学的研究对象是"系统"自身，其目的是探索各类系统的结构、环境与功能的普适关系以及演化与调控的一般规律。因为系统具有多层次性，所以如何阐述在每一个层次的一般规律及其层次之间的过按规律是系统科学作为一门学科的重要使命，而后者是系统科学最重要的任务。系统科学的核心是系统论。在历史上，物理学传统的还原论思想尽管已经做出了重要贡献，但复杂系统问题的分析与理解不能只停留在还原论层面上。与还原论相对应的是整体论，即需要将复杂系统作为一个整体来加以研究。系统的功能一般不能还原为其不同组分自身功能的简单相加，这
种1+1 不等于2 的结果称为涌现。

## 时间之箭
### 时间之箭问题
时间之箭(arrow of time)通常指的是时间的单向流动性，物理系统的时间反演的不对称性或不可逆性。在自然界中，宏观尺度的不可逆性随处可见，例如木头燃烧成灰烬、墨水滴入水中。或许是源自对这类现象的观察，人类对过去、现在和未来的区分有着明确的心理感受。当科幻作家和电影导演尝试利用因果颠倒、时光倒流来迷惑我们时，我们不仅会产生特有的奇异感和错愕感，还会感受到一种微妙的缘于宿命的忧伤和诗意。\par 
在物理学中，热力学第二定律可以将这种心理感受精确化和数学化。该定律表明，当孤立系统演化时，熵不会减少。因此熵就就相当于一支时间之箭，我们测量熵孤立系统的熵的变化就能够区分该系统的过去和未来。从统计力学的角度看，熵增定律是一条宏观的统计规律，为了描述一个由大量微观系统组合而成的宏观系统，我们必须忽略微观演化的许多复杂细节，经过统计平均后得到数量少得多的热力学量。然而，宏观系统不是微观系统的叠加和简化，而是与微观系统有着截然的区别。典型的例子就是，尽管微观系统的动力学具有可逆性，但是熵增定律告诉我们宏观系统的演化是不可逆的。探索时间之箭的本质问题就相当于在问：为什么热力学第二定律是成立的？ 这是对热力学定律的本质的追问。\par 
时间之箭的本质问题长久以来都作为科学和哲学中悬而未决的问题而为人津津乐道，激发着人们的想象力与创造力。在微观尺度上，由于观测对量子态会造成干扰，Kolmogorov相容性条件无法满足，我们无法使得物理量的潜在动力学满足一个经典随机过程，而只能用量子随机过程[[RN9344\|RN9344]]描述系统的动力学。根据这个想法，Lynn的工作或许可以衍生出相应的量子的时间箭头分解。在宏观尺度上，生命系统在酶的催化下，进行各种各样的生化反应使自己远离平衡态，新陈代谢和生老病死呈现出不可逆性，那么生命系统与非生命系统的不可逆性存在何种关联和区别[[gne\|gne]]？更进一步考虑，神经系统是生命中非常高级和特殊的部分，人们对过去和未来的区分有着明确的心理感受，热力学定律的给出的时间箭头和心理时间箭头之间存在着何种关联呢[[allori\|allori]]？类似的问题在Lynn的案例中也得到了体现，对于神经系统来说，无论输入的视觉刺激是否可逆，神经元的动力学都表现出了不可逆性。在宇观尺度上，我们会思考宇宙整体的时间箭头是否和热力学的宏观时间箭头存在关联。Hawking[[hawking1993\|hawking1993]]指出，热力学的时间箭头不会随着宇宙的膨胀和收缩发生变化，为了解释宇宙箭头和热力学箭头的一致性，我们必须诉诸弱人存原理。关于量化系统不可逆性、刻画时间之箭的讨论还有很多[[RN11061,RN11530\|RN11061,RN11530]]，可以预见，时间之箭的本质问题将是一个长盛不衰的问题，一直吸引我们向它发起挑战。\par 
实验物理学家Muller的科普书[[muller2016now\|muller2016now]]等材料{（[[mersini-houghton2012ArrowsTime\|mersini-houghton2012ArrowsTime]] p.8）}对不同种类的时间之箭总结如下
> [!abstract] Contents
> - 熵增箭头
> - 黑洞箭头
> - 辐射箭头
> - 心理箭头
> - 人存原理箭头
> - 量子物理中的时间反演破坏
> - 宇宙箭头

### 论文阅读笔记
一般认为，正是微观系统之间的相互作用导致了不可逆的宏观现象的产生。Lynn及其同事[[lynn2022pre,lynn2022prl\|lynn2022pre,lynn2022prl]]研究了系统之间的相互作用是否对时间之箭有贡献，如果答案为是，那么两体、三体，甚至更高阶关联的贡献分别又是多少。他们用随机过程对系统建模，将时间箭头数学化，并建立了在多自由度系统中分解时间箭头的局部证据(local evidence)的方法。局部不可逆性可以分为两个非负分量：一个反映了单个元素的独立演化的不可逆性，另一个反映了元素之间相互作用的不可逆性。相互作用项可以进一步分解成一些非负的不同阶的关联项，从两体关联到三体关联再到更高阶的关联。通过这种方式，人们不仅可以确定时间之箭是否源自元素之间的相互作用，还能确定时间之箭究竟产生于哪一阶的动力学。
#### 局部不可逆性和多体动力学
将有拥多个自由度的系统的动力学演化视为一个离散随机过程，可以定义联合转移概率，即从离散随机过程中取出两个相邻时刻构成联合分布
$$
P\left(x \rightarrow x^{\prime}\right) \equiv \operatorname{Prob}\left[x_t=x, x_{t+1}=x^{\prime}\right]
$$
由于只考虑了两个时刻，而不是多个时刻甚至连续的时间段上的联合转移概率，因此文中将由上式刻画的时间之箭称为“局部”时间之箭，以此与整体的时间之箭相区分。\par 
为了体现演化的不可逆性，可以用正向演化和反向演化的联合转移概率之间的Kullback-Leibler散度\footnote{又称相对熵，参见第\ref{sec:quantuminfobasic}节。}来量化局部时间之箭。
$$
\dot{I}=\sum_{x, x^{\prime}} P\left(x \rightarrow x^{\prime}\right) \log \left[\frac{P\left(x \rightarrow x^{\prime}\right)}{P\left(x^{\prime} \rightarrow x\right)}\right]
$$
KL散度描述了两个概率分布的差别有多大，当两个分布相同时，他们之间的KL散度为零，其他的主要性质包括非负性和不对称性。根据这些性质可以对上式进行一些直观的阐释，在可逆系统中，两个状态之间的转移概率没有净通量，系统的动力学满足细致平衡，正向演化和反相演化有着相同的分布，没有时间箭头的证据。在不可逆系统中，状态之间存在净通量，打破了细致平衡，建立了时间之箭。至于据此定义的时间之箭与熵增定律的关系，我还没有想明白，例如是否时间之箭大于零在数学上等价于熵增。\par 
对较小的封闭系统，我们通常能够完整地描述其动力学，例如经典系统的哈密顿正则方程、量子系统的薛定谔方程，这类动力学演化都是确定的或者在量子意义下是马尔科夫的。对于经典的马尔科夫系统，上述定义的联合转移概率确定了整个动力学的演化。实际上，时齐(time-homogeneous)的马尔科夫链的有限维联合分布可以用初始分布和一步条件转移概率确定[[amir2020\|amir2020]]。对于开放系统，我们通常不能观察系统和环境所组成的总系统的所有子系统或总系统的所有状态，这时可以用随机过程、隐马尔科夫链、主方程[[breuer2002,kubo2012\|breuer2002,kubo2012]]、Nakajima-Zwanzig方程[[Zwanzig\|Zwanzig]]等方法描述我们所关心的系统。这些动力学演化可能不再是马尔科夫的。完整的动力学往往是确定性的或马尔可夫的，而约化动力学往往和非马尔可夫的，对这些系统做马尔科夫近似意味着对时间做粗粒化近似[[breuer2002\|breuer2002]]。\par 
假设我们感兴趣的系统由$N$个子系统构成，只要对子系统的局部不可逆性进行合适的定义，总的局部不可逆性可以分解为子系统的局部不可逆性的和
$$
\dot{I}=\sum_{i=1}^{N}\dot{I}_i
$$
#### 独立与相互作用的不可逆性
通过定义边缘转移概率，
$$
P(x_i\rightarrow x_i^{\prime})=\sum_{x_{-i}}P(x_i\rightarrow  x_i^{\prime},x_{-i})
$$
每个子系统的局部不可逆性可以进一步分解为自身的独立不可逆性与相互作用不可逆性
$$
\dot{I}_i^{\rm ind}=\sum_{x_i, x_i^{\prime}} P\left(x_i \rightarrow x_i^{\prime}\right) \log \left[\frac{P\left(x_i \rightarrow x_i^{\prime}\right)}{P\left(x_i^{\prime} \rightarrow x_i \right)}\right]
$$
其中$P(x_i\rightarrow  x_i^{\prime},x_{-i})$表示第$i$个子系统转移，而其他系统的状态保持不变的联合转移概率。\par 
可以证明相互作用不可逆性
$$
\dot{I}_{\mathrm{i}}^{\text {int }}=\dot{I}_{\mathrm{i}}-\dot{I}_{\mathrm{i}}^{\text {ind }}
$$
总是大于等于零。
对于总系统来说，有
$$
\dot{I}=\dot{I}^{\text {ind }}+\dot{I}^{\text {int }}
$$
从这个分解可以得到结论：系统内部的相互作用总是增加系统的局部不可逆性。
#### 相互作用不可逆性的分解
本节讨论开头提出的问题，即，为了计算整个系统的不可逆性，我们是只需要知道所有二体约化系统的动力学演化，还是需要知道更高阶的约化系统动力学。首先考虑两体的约化动力学，假设给定系统满足边缘转移概率
$$
\begin{aligned}
&P\left(x_{\mathrm{i}} \rightarrow x_{\mathrm{i}}^{\prime}, x_{\mathrm{j}}\right)=\sum_{x_{-[i, \mathrm{j}]}} P\left(x_{\mathrm{i}} \rightarrow x_{\mathrm{i}}^{\prime}, x_{-\mathrm{i}}\right) \\
&P\left(x_{\mathrm{j}} \rightarrow x_{\mathrm{j}}^{\prime}, x_{\mathrm{i}}\right)=\sum_{x_{-\{i, j]}} P\left(x_{\mathrm{j}} \rightarrow x_{\mathrm{j}}^{\prime}, x_{-\mathrm{j}}\right)
\end{aligned}
$$
如果我们在上式的约束下，遍历系统的所有可能动力学演化，找到其中使得不可逆性最小的一个，将最小值记为$\dot{I}^{(2)}$。直觉上，由于约化动力学仅体现了系统的部分不可逆性，因此$\dot{I}^{(2)}$是$\dot{I}$的一个下界。依次考虑更高阶的约化系统得到局部不可逆性的一系列递增的下界
$$
0 \leq \dot{I}^{(1)} \leq \dot{I}^{(2)} \leq \cdots \leq \dot{I}^{(N-1)} \leq \dot{I}^{(N)}=\dot{I}
$$
只要定义$\dot{I}_{\text {int }}^{(k)}=\dot{I}^{(k)}-\dot{I}^{(k-1)}$
就可以将局部不可逆性拆解成不同阶的约化系统的相互作用不可逆性的和
$$
\dot{I}=\underbrace{\dot{I}_{\mathrm{int}}^{(1)}}_{i^{\text {ind }}}+\underbrace{\dot{I}_{\mathrm{int}}^{(2)}+\dot{I}_{\mathrm{int}}^{(3)}+\cdots+\dot{I}_{\mathrm{int}}^{(N)}}_{i^{\text {int }}},
$$
这是Lynn及其同事的文章中的最重要的结论和创新之处。
#### 应用
上述理论可以应用于一些有趣的系统，文中举了带噪声的逻辑系统和神经系统两例。通过分解这些系统的不可逆性，我们可以不可逆性是否源自系统内部的相互作用，如果是的话，又需要多少阶的约化动力学才能描述系统的不可逆性。\par 
对于带噪声的逻辑系统，每个逻辑门有两个输入和一个输出变量，每个变量都有两种状态，共计八种。经过计算，对于复制函数，不可逆性完全来自二阶动力学；对于与运算和或运算不可逆性来自二阶和三阶动力学的组合；对于异或运算，不可逆性完全来自三阶动力学。\par 
通过分析蝾螈对复杂视觉刺激的神经响应，讨论不可逆性是否来自于神经元之间的相互作用。 本文讨论了实验中使用的不同类型的视觉输入。第一种类型的输入是自然影片，能够容易地看出正向播放和倒放的区别。第二种类型的输入是垂直移动的单个水平条，运动轨迹是弹簧上的布朗粒子。第三种是第二种运动轨迹的多次重复。第一种属于不可逆刺激，后两种属于可逆刺激。根据一段时间内是否产生动作电位，用0和1标记每个神经元，用一串二进制数表示一组神经元。主要结论包括：观察到3个神经元的细致平衡被破坏，神经元的工作状态是非平衡稳态；神经元的不可逆性与外界刺激有关，但是无论视觉刺激是否可逆，神经元都表现出不可逆性；神经元的不可逆性主要来自二阶的动力学。
#### 总结
Lynn及其同事的工作用KL散度来衡量时间之箭，并将多自由度的系统的局部不可逆性分解成独立不可逆性与各阶约化动力学的相互作用不可逆性。时间之箭的分解提供了一个一般性的框架，使得人们能够解释系统内复杂的相互作用如何导致了演化的不可逆性。\par 

## 动力系统
### 一维动力系统
作为高维动力系统的原型，简单的非线性一维动力系统为我们提供了一个观察复杂系统行为窗口。
> [!abstract] Contents
> - 一维映射的不动点
> - 不动点的局域和全局稳定性
> - 绘制蛛网图
> - Logistic映射是一个非常简单的非线性动力系统，但是其中蕴藏了很多复杂与混沌的行为
> $$x_{n+1}=a x_{n}\left(1-x_{n}\right)$$
>   从倍周期分岔点数列$\{a_n\}$可以得到一维映射的Feigenbaum常数[[Feigenbaum\|Feigenbaum]]
>$$\delta=\lim \limits_{n \rightarrow \infty} \frac{a_{n-1}-a_{n-2}}{a_{n}-a_{n-1}}=4.669201609 \ldots$$>   相继两个倍周期分岔点的间隔指数减小。
> - 正的Lyapunov指数意味着动力学映射将微小扰动不断放大，这通常意味着混沌，动力学$x_{n+1}=f\left(x_{n}\right)$的Lyapunov指数定义如下
>$$\lambda=\lim \limits_{n \rightarrow \infty} \frac{1}{n} \sum_{i=0}^{n-1} \ln | f^{\prime}\left(x_{i}\right)|$$
> 在一般动力学系统中，最大Lyapunov指数
> $$\lambda=\lim \limits_{t \rightarrow \infty} \lim \limits_{\left|\delta \mathbf{Z}_{0}\right| \rightarrow 0} \frac{1}{t} \ln \frac{|\delta \mathbf{Z}(t)|}{\left|\delta \mathbf{Z}_{0}\right|}$$
> 其中，$\delta \mathbf{Z}_{0}$是初始的微小扰动。

### 不动点及其稳定性
设一维离散时间动力系统$x_{n+1}=f(x_n)$有不动点$x^*$，则$x^*$局部稳定的充分条件是$|f'(x^*)|<1$，
设一维连续时间动力系统$\dot{x}=f(x)$有不动点$x^*$，则$x^*$局部稳定的充分条件是$|f'(x^*)|<0$，更一般地，对高维的线性系统有以下命题
> [!proposition]
> - {线性系统的稳定性}
>   一阶矩阵ODE
>
$$
\dot{\mathbf{x}}(t)=\mathbf{A}(t) \mathbf{x}(t)
$$
>   当$\mathbf{A}$不含时，且有$n$个线性独立的本征值时
>
$$
\mathbf{x}(t)=c_{1} e^{\lambda_{1} t} \mathbf{u}_{1}+c_{2} e^{\lambda_{2} t} \mathbf{u}_{2}+\cdots+c_{n} e^{\lambda_{n} t} \mathbf{u}_{n}
$$
>   解是稳定的当且仅当所有本征值的实部为负数。

> [!note]
> - 动力学系统最简单的解是不动点解。分析动力系统稳定性的一般方法是在动力系统的平衡点附近做线性近似，再根据以上命题分析线性系统的稳定性。

下面以二变量常微分方程为例来考虑不动点的可能类型及其稳定性。\par
$$
\frac{\mathrm{d} u_{1}}{\mathrm{~d} t}=f_{1}\left(u_{1}, u_{2}\right), \quad \frac{\mathrm{d} u_{2}}{\mathrm{~d} t}=f_{2}\left(u_{1}, u_{2}\right)
$$
联立$f_1=0,f_2=0$得到不动点解$(u_1^0,u_2^0)$，对方程线性化得到一阶ODE
$$
\frac{\mathrm{d}}{\mathrm{d} t}\left(\begin{array}{c}\delta u_{1} \\ \delta u_{2}\end{array}\right)=\boldsymbol{A}\left(\begin{array}{c}\delta u_{1} \\ \delta u_{2}\end{array}\right)
$$
其中
$$
A_{i j}=\left.\frac{\partial f_{i} }{\partial u_{j}}\right|_{\left(u_{1}^{0}, u_{2}^{0}\right)}, \quad i, j=1,2
$$
设
$$
\begin{array}{c}T=\operatorname{Tr}(\boldsymbol{A})=A_{11}+A_{22} \\ \Delta=\operatorname{det} \boldsymbol{A}=A_{11} A_{22}-A_{21} A_{12}\end{array}
$$
\par 
> [!abstract] Contents
> - 当 $T^{2}-4 \Delta \geqslant 0$ 时, $\lambda_{1}, \lambda_{2}$ 为一对实数解，不动点解 $\left(u_{1}^{0}, u_{2}^{0}\right)$ 有以下三类:
>
> - 当 $\lambda_{1}<0, \lambda_{2}<0$ 时, $\left(u_{1}^{0}, u_{2}^{0}\right)$ 附近所有方向都局域稳定, 不动点为稳定结 点 (stable node, sink)
> - 当 $\lambda_{1}>0, \lambda_{2}>0$ 时, $\left(u_{1}^{0}, u_{2}^{0}\right)$ 为不稳定结点 (unstable node, source)
> - 当 $\lambda_{1}<0, \lambda_{2}>0$ 或 $\lambda_{1}>0, \lambda_{2}<0$ 时, 不动点附近有一个不稳定方 向, $\left(u_{1}^{0}, u_{2}^{0}\right)$ 为鞍点 (saddle point)
> - 当 $T^{2}-4 \Delta<0$ 时, $\lambda_{1}, \lambda_{2}$ 为一对共轭复根, 扰动随时间的演化是振荡的, 不动点解 $\left(u_{1}^{0}, u_{2}^{0}\right)$有以下三类:
>
> - 当 $T=2 \operatorname{Re}\left(\lambda_{1,2}\right)<0$ 时, $\left(u_{1}^{0}, u_{2}^{0}\right)$ 为稳定焦点 (stable focus, spiral source)
> - 当 $T>0$ 时, $\left(u_{1}^{0}, u_{2}^{0}\right)$ 为不稳定焦点 (unstable focus, spiral sink)
> - 当 $T=0$ 时, $\left(u_{1}^{0}, u_{2}^{0}\right)$ 为中心点 (center)。对于中心点, 仅凭线性稳定性分析不足以揭示其稳定性, 需进一步考察高阶非线性的稳定性。

### 分岔 Bifurcation
通常将动力学系统在系统参数变化时发生的解的产生与消失及其稳定性变化称为分岔。
> [!abstract] Contents
> - 鞍结分岔(saddle-node bifurcation)：一个不动点变成一个稳定不动点和一个不稳定不动点。
>   描述该分岔的特征方程是
>
$$
\dot{u}=R-u^{2}
$$
>   当$R<0$时，不存在不动点。当$R>0$时，不动点$u_0=\pm \sqrt{R}$。
>   设$u=u_0+\delta$，代入特征方程可得
>
$$
\dot{\delta}=-2u_o\delta
$$
>   因此系统的分岔点是$R_{c}=0$，当 $R$ 由负变正时,系统出现稳定的结点$u_{0}=\sqrt{R}$，
>   另一个结点 $u_{0}=-\sqrt{R}$ 不稳定。
> - 跨临界分岔(transcritical bifurcation)：原来稳定的解失稳，原来不稳定的解变得稳定。
>   特征方程为
>
$$
\dot{u}=R u-u^{2}
$$
>   当 $R$由负变正时, 原来稳定的解$u_{0}=0$ 失稳, 原来不稳定的解$u_{0}= R$变得稳定。
> - 叉式分岔(pitch-fork hifurcation):当参量改变时，原来的单解失稳，出现两支稳定的新解。
>   典型方程
>
$$
\dot{u}=R u-g u^{3}, \quad g>0
$$
>   当$R<0$时，有唯一不动点$u_0=0$，该不动点稳定，当$R$由负边正，$u_0=0$失稳，当 $R>0$ 时出现两支稳定的新解 $(R<0$ 时不存在)
>
$$
u_{0}= \pm \sqrt{R / g}
$$
>   若当参数改变时，系统的解分支连续变化，则称分岔为超临界的(supercritical)，对应相变理论中的二级(连续)相变; 当系统只有分支的失稳时，称分岔为亚临界的(subcritical)，对应一级(不连续)相变。
> - Hopf 分岔(Hopf hifurcation)：原来的不动点解失稳后出现随时间振荡的解，称为极限环(limit cycle) 。\par

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{image//2023-04-21-21-41-13.png}
    \caption{ Stability diagram }
    \label{  }
   \end{figure}
   
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{image//bifur4pic.pdf}
    \caption{ (a) 鞍结分岔， (b) 跨临界分岔， (c) 叉式分岔， (d) Hopf 分岔 }
    \label{  }
   \end{figure}
   
   
### Lorenz吸引子\footnote{注意不是Lorentz}
Lorenz系统
$$
\begin{array}{l}\frac{\mathrm{d} x}{\mathrm{~d} t}=\sigma(y-x) \\ \frac{\mathrm{d} y}{\mathrm{~d} t}=x(r-z)-y \\ \frac{\mathrm{d} z}{\mathrm{~d} t}=x y-\beta z\end{array}
$$
当$r<1$时，原点是唯一的不动点，构造Lyapunov函数可以证明，它是全局稳定点（全局吸引子）。$r=1$时发生pitchfork分岔，$1<r$时，有三个不动点$(x,y,z)=(0,0,0),(-\sqrt{b} \sqrt{r-1},-\sqrt{b} \sqrt{r-1},r-1),(\sqrt{b} \sqrt{r-1},\sqrt{b} \sqrt{r-1},r-1)$。当$1<r_\mathrm{H}$时，不动点是稳定的，当$r=r_\mathrm{H}$时，发生Hopf分岔，此后出现极限环。其中Hopf分岔点（复根穿过虚轴的的参数点）是
$$
r_\mathrm{H}=-\frac{\sigma(3+b+\sigma)}{1+b-\sigma}
$$
\par 
  当$\sigma,r>0$时，相空间体积收缩
$$
\frac{\mathrm{d} V}{\mathrm{~d} t}=\int_{V} \mathrm{~d} x \mathrm{~d} y \mathrm{~d} z\left(\frac{\partial f_{1}}{\partial x}+\frac{\partial f_{2}}{\partial y}+\frac{\partial f_{3}}{\partial z}\right)=-(\sigma+1+b) V<0
$$
这意味着Lorenz系统是耗散系统，其吸引子的维度一定小于3。

## 分形 Fractal
参考[[suslick2001encyclopedia\|suslick2001encyclopedia]]。自然界中普遍存在具有粗糙和零碎的几何形状的物体，称为分形，它们往往具有所谓的“标度不变性”，即局部与整体的相似性，将分形无穷无尽地放大仍将得到与原来类似的精细的几何结构。
### 分形的自相似性
如果一个形状$S$能拆成$n$份$S=S_{1} \cup S_{2} \cup \ldots \cup S_{n}$且$S_i$是$S$线性缩小到原来的$1/r_i$倍得到的，且$S_i$的重叠部分足够小，那么称$S$是**精确线性自相似**的，分形的**相似维度**$d$刻画了$S$的粗糙程度，$d$满足方程
$$
\sum_{i=1}^{n} r_{i}^{d}=1
$$
当缩放因子都相同时，
$$
d=\frac{\log(n)}{\log(1/r)}
$$
数学上有许多优雅的方法生成完全线性自相似分形，例子有Sierpinksi三角、Peano曲线等。当形状的缩放推广到非线性时，称其为完全非线性自相似分形，这样的分形包括Mandelbrot集与Julia集。\par 
自然界中也存在许多不是精确自相似的形状，为了描述这种分形的自相似性，需要引入统计学方法。
### 分形的维度
> [!definition]
> - {盒维度}$n$维欧式空间中的有界子集$A$，$N_{\delta}(A)$是用半径为$\delta$开球覆盖$A$的所需的最少的开球数，称
>
$$
d_{\mathrm{box}}=\lim \limits_{\delta \rightarrow 0} \frac{\log \left(N_{\delta}(A)\right)}{\log (1 / \delta)}
$$
>   为**盒维度**。

> [!definition]
> - {关联维度}在$n$维欧式空间上有由$N$个点组成的点集$A$，定义关联积分
>
$$
C(\delta)=\lim \limits_{N \rightarrow \infty} \frac{g}{N^{2}}
$$
>   其中$g$是距离小于$\delta$的点对数量。**关联维度**
>
$$
d_\mathrm{cor}=\lim_{\delta\rightarrow0}\frac{\log(C(\delta))}{\log(\delta)}
$$
> [!note]
> - 如何对给定的分形采样？

\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{image//2023-05-14-10-51-20.png}
    \caption{在Lorenz吸引子上采样}
    \label{fig:}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=\textwidth]{image//cordim.pdf}
    \caption{关联维度}
    \label{fig:}
    \end{subfigure}
    \caption{Lorenz的关联维度$d=2.09484$}
    \label{fig:}
\end{figure}
> [!definition]
> - {Hausdorff–Besicovitch维度}
>   $n$维欧式空间中的子集$A$，给定$d \geq 0$，$\delta\geq 0$，定义
>
$$
H_{\delta}^{d}(S)=\inf \left\{\sum_{i=1}^{\infty}\left(\operatorname{diam} U_{i}\right)^{d}:\bigcup_{i=1}^{\infty} U_{i} \supseteq S, \operatorname{diam} U_{i}<\delta\right\}
$$
>   称
$$
H^{d}(S)=\lim \limits_{\delta \rightarrow 0} H_{\delta}^{d}(S)
$$
>   为**Hausdorff–Besicovitch测度**。$S$的**Hausdorff维度**$d_\mathrm{H}$满足
>
$$
d<d_{\mathrm{H}} \quad \Rightarrow \quad H^{d}(S)=\infty
$$
>
$$
d>d_{\mathrm{H}} \quad\Rightarrow \quad H^{d}(S)=0
$$
### 固有维度 Intrinsic Dimensionality
参考文章[[ansuini2019intrinsic\|ansuini2019intrinsic]]。\par 
目前的深度神经网络中，过量的参数导致了模型的过参数化，带来了冗余信息，神经网络压缩就是由此产生的。那么，有没有什么定量的统计量来显示网络的复杂度或者评估网络呢，本文就是以固有维度为切入点来探索该统计量和模型泛化能力\footnote{ 泛化能力是指测试集上的表现能力，它反映了模型对新样本的适应能力和概括能力。}之间的关系。\par 
本文重点探讨深度神经网络 (DNN) 中数据表示的固有维度的概念。固有维度是指捕获数据集基本结构所需的最小变量数量。了解数据表示的固有维度对于深入了解 DNN 如何学习和泛化至关重要。研究人员进行了实验，测量在各种数据集（包括图像分类和自然语言处理任务）上训练的 DNN 中中间表示的固有维度。他们采用了现有的维度估计技术，例如相关维度、打包维度和基于最近邻的方法。

研究结果表明，中间表示的固有维度随着网络深度的增加而增加。这表明与较浅的层相比，DNN 中的较深层可以捕获更复杂和抽象的特征。研究人员还观察到，固有维度在训练过程中往往会增加，这表明网络逐渐发现数据的更细微的方面。此外，该研究还探讨了固有维度和泛化性能之间的关系。令人惊讶的是，他们发现两者之间没有直接相关性。虽然较高的固有维度表明模型的表达能力增强，但并不一定能保证更好的泛化能力。这些发现对于理解 DNN 的学习动态和优化其架构具有重要意义。通过深入了解数据表示的固有维度，研究人员可以开发更高效、更有效的模型，并有可能减少过度拟合。
\par 
下面的推导整理自[[facco2017estimating\|facco2017estimating]]。\par 
文章介绍了一种估计数据集固有维度的方法，称为TWO-NN，该方法仅使用样本中每个点的第一个和第二个最近邻的距离，这种极端的最小化能够节约计算成本。\par 
设$i$是数据集中的一个点，他嵌在$d$维空间中，$r_k$表示$i$到第$k$近邻的点之间的距离（约定$r_0=0$），第$l$和$l-1$近邻点所围的体积是
$$
\Delta v_{l}=\omega_{d}\left(r_{l}^{d}-r_{l-1}^{d}\right)
$$
其中，$\omega_d$是$d$维单位球的体积。假设点$i$附近的密度$\rho$是常数，以$i$为中心$d$为半径的球记为$B_{i,d}$，到$i$距离分别为$a,b$的两点组成球壳记为$C_{a,b}$。设点的分布服从泊松分布
$$
P (\text{Borel集$A$内含有$n$个点}) :=P(N(A)=n)=\frac{(\rho \mu(A))^{n}}{n !} e^{-\rho \mu(A)}
$$
且对于不相交的两个集合$A_1$与$A_2$，其中点的数量$N(A_1)$与$N(A_2)$是相互独立的随机变量。
其中，$\mu$是测度，由于$E[P(n, A)]=\rho \mu(A)$，故$\rho$是点的密度。最近邻点距离$d_1$处于$C_{r_1,r_2}$内的概率
$$
\begin{aligned} P\left(d_{1} \in C_{r_{1}, r_{1}+d r_{1}}\right) & =P\left(N\left(B_{i, r_{1}}\right)=0, N\left(C_{r_{1}, r_{1}+d r_{1}}\right) \geq 1\right) \\ & =P\left(N\left(B_{i, r_{1}}\right)=0\right) P\left(N\left(C_{r_{1}, r_{1}+d r_{1}}\right) \geq 1\right) \\ & =P\left(N\left(B_{i, r_{1}}\right)=0\right)\left(1-P\left(N\left(C_{r_{1}, r_{1}+d r_{1}}\right)=0\right)\right) \\ & =e^{-\rho r_{1}^{2} \pi}\left(1-e^{-\rho \pi r_{1} d r_{1}}\right) \\&
    \approx e^{-\rho r_{1}^{2} \pi} 2 \pi \rho r_{1} d r_{1}
\end{aligned}
$$
给定最近邻点的距离为$r_1$时，次近邻距离为$r_2$的概率
$$
\begin{aligned} 
    P\left(r_{2} \mid r_{1}\right)
    &=P\left(N\left(C_{r_{1}, r_{2}}\right)= 0, N\left(C_{r_{2}, r_{2}+d r_{2}}\right) \geq 1 \mid N\left(B_{i, r_{1}}\right)=0, N\left(C_{r_{1}, r_{1}+d r_{1}}\right) \geq 1\right) \\& =P\left(N\left(C_{r_{1}, r_{2}}\right)= 0 \mid N\left(B_{i, r_{1}}\right)=0, N\left(C_{r_{1}, r_{1}+d r_{1}}\right) \geq 1\right)  P\left(N\left(C_{r_{2}, r_{2}+d r_{2}}\right) \geq 1 \mid N\left(B_{i, r_{1}}\right)=0, N\left(C_{r_{1}, r_{1}+d r_{1}}\right) \geq 1\right) \\
   &= e^{-\rho \pi\left(r_{2}^{2}-r_{1}^{2}\right)}\left(1-P\left(N\left(C_{r_{2}, r_{2}+d r_{2}}\right)=0 \mid N\left(B_{i, r_{1}}\right)=0, N\left(C_{r_{1}, r_{1}+d r_{1}}\geq 1\right)\right)\right)\\
   & \sim e^{-\rho \pi\left(r_{2}^{2}-r_{1}^{2}\right)} 2 \rho \pi r_{2} d r_{2}
\end{aligned}
$$
联合概率分布
$$
P\left(r_{1}, r_{2}\right)=P\left(r_{2} \mid r_{1}\right) P\left(r_{1}\right) \sim e^{-\rho \pi r_{2}^{2}}(2 \rho \pi)^{2} r_{1} r_{2} d r_{1} d r_{2}
$$
$$
P\left(r_{1}, r_{2}, \ldots, r_{n}\right) \sim e^{-\rho \pi r_{n}^{2}}(2 \rho \pi)^{n} r_{1} r_{2} \cdots r_{n} d r_{1} d r_{2} \cdots d r_{n}
$$
从$(r_1,\cdot,r_n)$的pdf可以得到$\left(\Delta v_{1}, \Delta v_{2}, \ldots, \Delta v_{n}\right)=\left(\pi r_{1}^{2}, \pi\left(r_{2}^{2}-r_{1}^{2}\right), \ldots, \pi\left(r_{n}^{2}-r_{n-1}^{2}\right)\right)$的pdf
$$
g\left(\Delta v_{1}, \Delta v_{2}, \ldots, \Delta v_{n}\right)=\rho^{n} e^{-\rho\left(\Delta v_{1}+\Delta v_{2}+\ldots+\Delta v_{n}\right)}
$$
综上所述，$\Delta v_l$服从指数分布
$$
P\left(\Delta v_{l} \in[v, v+d v]\right)=\rho e^{-\rho v} d v
$$
由此可得$R:=\frac{\Delta v_2}{\Delta v_1}$的pdf
$$
g(R)=\frac{1}{(1+R)^{2}}
$$
定义$\mu:=\frac{r_2}{r_1}\in[1,+\infty)$，则有$R=\mu^{d}-1$。通过简单的计算可得$\mu$的pdf与cdf分别为
$$
f(\mu)=d\times  \mu^{-d-1}
$$
$$
F(\mu)=\left(1-\mu^{-d}\right)
$$
终于，我们得到了固有维度的估计公式
$$
d=\frac{\log (1-F(\mu))}{\log (\mu)}
$$
## 自组织临界 Self-organizing Towards Criticality

整理自文章[[shew2015soc\|shew2015soc]]\footnote{模拟自组织临界：SOC 自组织临界1.nb}。
> [!abstract] Contents
> - 幂率得到满足意味着系统处于临界点。在临界点，神经网络表现出多方面的幂率，使它们能够快速而稳健地响应感官输入的变化，同时避免过度兴奋和兴奋不足。
> - 相变的临界点是物理学中的一个概念，指的是系统在从一个相转变为另一个相时行为发生突然变化的点。在大脑神经网络的背景下，这个临界点被认为代表了信息处理的最佳状态。
> - 自组织是指大脑中的神经网络在感觉信息处理过程中自发地组织自己进入临界状态的能力。
> - 强烈的感官输入最初会引发并不重要的皮层网络动态，但网络中的适应性变化会迅速将系统调整到临界状态。
> - 神经元雪崩是一种发生在大脑中的自发活动，其特征是级联的神经元放电以自组织的方式通过网络传播。
> - 临界现象和幂率以及长程关的关系？
>   \begin{myitemize}
> - 在物理系统中，波动是指描述系统行为的不同可观测值或变量值的微小随机变化。这些波动可能发生在不同的空间或时间尺度上，并且它们通常相互关联。
> - 当系统接近临界点时，这些波动在长距离或时间尺度上变得相关。这意味着系统中某一点的小波动会影响远离它的其他点的行为。这些相关性在各种物理可观测值 （例如相关函数、磁化率和响应函数） 中产生幂律行为。例如，考虑一种接近其临界温度的铁磁材料。在此温度下，材料经历从磁化状态到非磁化状态的相变。当我们接近临界温度时，材料中长距离的小磁波动变得相关。这导致各种磁性可观测值的幂律行为，例如磁化率和相关长度。
> - 幂律行为的出现是因为波动之间的这些相关性导致自相似模式在不同的长度或时间尺度上重复出现。这些模式由独立于微观细节且仅取决于系统的对称性和维数的通用比例定律来描述。
> - 幂律行为的出现是由于物理系统在长距离或时间尺度上的波动之间的相关性。这些相关性会产生自相似模式，这些模式会在不同的长度或时间尺度上重复出现，并由临界点附近的通用尺度定律来描述。
> - 热统中的临界指数描述了临界点附近各种物理可观测值的幂律行为。
> - 考虑一个表现出自组织临界性的沙堆模型。在这个模型中，小的扰动会引发更大的沙崩，从而导致自相似的活动模式。随着雪崩规模的增加，活动的持续时间也增加，但活动的形状和结构保持相似。这种自相似性的出现是因为系统中的波动在不同的长度和时间尺度上是相关的。例如，沙堆一部分的小扰动会引发连锁反应，导致沙堆其他部分发生更大的雪崩。这些相关性导致在不同尺度上重复自身的模式，从而导致雪崩大小和频率之间的幂律比例关系。

\end{myitemize}

## 神经网络
### 概述
学习的种类：监督学习、无监督学习、强化学习
### 训练神经网络
反向传播算法
### 深度强化学习

### 深度学习的理论研究

这篇文章[[schoenholz2016deep\|schoenholz2016deep]]使用平均场理论研究未经训练的神经网络的行为，其权重和偏差是随机分布的。文章指出，特征深度(depth scales)的存在自然地限制了通过这些随机网络的信号传播的最大深度。其主要结论是，当信息能够穿过神经网络时恰好意味着他们可以被训练。由此，只要渐近深度神经网络的初始化足够接近有序到混沌的转变，特征深度就无限长，她就应该是可训练的。\par 

考虑一个全连接、未训练的前馈神经网络，深度为$L$，层宽$N_l$，权重和偏置独立同分布
$$
W_{i j}^{l} \sim N\left(0, \sigma_{w}^{2} / N_{l}\right)~~b_{i}^{l} \sim N\left(0, \sigma_{b}^{2}\right)
$$
信号传输由下面的方程描述
$$
z_{i}^{l}=\sum_{j} W_{i j}^{l} y_{j}^{l}+b_{i}^{l} \quad y_{i}^{l+1}=\phi\left(z_{i}^{l}\right)
$$
平均场近似是指用Gauss分布近似替代$z_i^l$，对于一对输入信号$y_i^0=x_{i ; a}^{0}$ 与$x_{i ; b}^{0}$，平均场近似是
$$
\mathbb{E}\left[z_{i ; a}^{l}\right]=0,~\mathbb{E}\left[z_{i ; b}^{l}\right]=0
$$
$$
\mathbb{E}\left[z_{i ; a}^{l} z_{j ; a}^{l}\right]=q_{a a}^{l} \delta_{i j}，~~\mathbb{E}\left[z_{i ; b}^{l} z_{j ; b}^{l}\right]=q_{b b}^{l} \delta_{i j}，~~\mathbb{E}\left[z_{i ; a}^{l} z_{j ; b}^{l}\right]=q_{a b}^{l} \delta_{i j}
$$
其中，方差和协方差满足迭代关系
$$
q_{a a}^{l}=\sigma_{w}^{2} \int \mathcal{D} z \phi^{2}\left(\sqrt{q_{a a}^{l-1} z}\right)+\sigma_{b}^{2}
$$
eq-eq-varrec
$$
q_{a b}^{l}=\sigma_{w}^{2} \int \mathcal{D} z_{1} \mathcal{D} z_{2} \phi\left(u_{1}\right) \phi\left(u_{2}\right)+\sigma_{b}^{2}
$$
eq-eq-covrec

其中$u_{1}=\sqrt{q_{a a}^{l-1}} z_{1}$ and $u_{2}=\sqrt{q_{b b}^{l-1}}\left(c_{a b}^{l-1} z_{1}+\sqrt{1-\left(c_{a b}^{l-1}\right)^{2}} z_{2}\right)$，传播$l$层后两个数据的关联
$$
c_{a b}^{l}=q_{a b}^{l} / \sqrt{q_{a a}^{l} q_{b b}^{l}}
$$
设$q^*$是\eqref{eq:varrec}的不动点。
$c^*=1$是\eqref{eq:covrec}的不动点，为研究不动点的稳定性需要引入
$$
\chi_{1}=\frac{\partial c_{a b}^{l}}{\partial c_{a b}^{l-1}}=\sigma_{w}^{2} \int \mathcal{D} z\left[\phi^{\prime}\left(\sqrt{q^{*}} z\right)\right]^{2}
$$
$\chi_1<1$意味着稳定，反之不稳定。\par 
 对于足够大的深度$l$，期望$\left|q_{a a}^{l}-q^{*}\right| \sim e^{-l / \xi_{q}}$ and $\left|c_{a b}^{l}-c^{*}\right| \sim e^{-l / \xi_{c}}$渐进成立$\xi_q$与$\xi_c$称为特征深度，分别描述了单个数据和两个数据输入的信息传播深度。\par 
 设$q_{a a}^{l}=q^{*}+\epsilon^{l}$，可得迭代关系与特征深度$\xi_q$
 
[[depthscale.pdf]]
相图和特征深度：(a)有序和混沌相分别对应着梯度消失和梯度爆炸
$$
\epsilon^{l+1}=\epsilon^{l}\left[\chi_{1}+\sigma_{w}^{2} \int \mathcal{D} z \phi^{\prime \prime}\left(\sqrt{q^{*}} z\right) \phi\left(\sqrt{q^{*}} z\right)\right]+\mathcal{O}\left(\left(\epsilon^{l}\right)^{2}\right)
$$
$$
\xi_{q}^{-1}=-\log \left[\chi_{1}+\sigma_{w}^{2} \int \mathcal{D} z \phi^{\prime \prime}\left(\sqrt{q^{*}} z\right) \phi\left(\sqrt{q^{*}} z\right)\right]
$$
设$c_{a b}^{l}=c^{*}+\epsilon^{l}$，可得迭代关系与特征深度$\xi_c$
$$
\xi_{c}^{-1}=-\log \left[\sigma_{w}^{2} \int \mathcal{D} z_{1} \mathcal{D} z_{2} \phi^{\prime}\left(u_{1}^{*}\right) \phi^{\prime}\left(u_{2}^{*}\right)\right]
$$
在有序相中$c^*=1$，因此$\xi_{c}^{-1}=-\log \chi_{1}$。在有序相中，$\chi_1<1$，特征深度大于零，梯度将在特征深度出消失；在临界点$\chi_1\rightarrow 1$，特征深度无穷大，梯度将保持稳定，在混沌相中$\chi_1>1$，特征深度小于零，梯度将在特征深度的绝对值出爆炸。处于混沌边缘的神经网络是最容易训练的。

# 松散的内容

- [Faster than Light in Our Model of Physics: Some Preliminary](Mathematica.md)