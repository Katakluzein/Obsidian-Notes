---
{"dg-publish":true,"permalink":"/1 Stories of Bizzo's Life/5 量子科学和量子技术/28 量子信息/","tags":["量子信息"]}
---


> [!quote] 
> I take the view that we all have permission to be a little baffled by quantum information science and algorithmic information theory.
> —— James Gleick
{ #0j0nuy}


**摘要**
- 相对熵

## 经典信息论

[[经典信息论\|经典信息论]]
## 量子信道 Quanutum Channel

[[1 Stories of Bizzo's Life/5 量子科学和量子技术/量子信道\|量子信道]]
## 量子通信简介

量子通信承诺可靠地传输量子信息，有效地分发纠缠和产生完全安全的密钥。在隐形传态任务中，如何A和B预先共享一个纠缠态，那么就可以通过经典通信的方式来实现任意量子比特的传输，由于噪声的存在共享后的纠缠态往往不是纯的，我们需要从大量不纯的纠缠态中蒸馏得到少量纯的纠缠态，以提高量子态的传输速率。在量子通信中，一个重要的任务是确定在无中继器且不限制局域操作和双向经典通信的情况下的点到点量子传输速率，根据目标态的不同，可以定义不同类型的量子容量，例如双向量子容量(two-way quantum capacity) (pirandola2017FundamentalLimitsRepeaterlessa)。

## 信息论

好的，我将根据您提供的图片内容，并参考《The Theory of Quantum Information》这本教材，按顺序整理出一份完整的量子信息讲义。每次输出一页的内容。

***

### **量子信息讲义 - 第 1 页 / 共 13 页**

---

### **第一部分：经典熵函数 (Classical Entropic Functions)**

本节回顾信息论中的基本概念，包括联合熵、条件熵、相对熵和互信息，并探讨它们的核心性质。这些概念是后续学习量子熵理论的基础。

#### **1. 联合熵 (Joint Entropy)**

联合熵衡量一对随机变量 (X, Y) 的不确定性总和。

**定义:** 设 (X, Y) 是遵循联合概率分布 $p(x, y)$ 的随机变量，其联合熵定义为：
$$
H(X, Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log_2 p(x, y)
$$
也可以写成期望值的形式：
$$
H(X, Y) = \mathbb{E}[-\log_2 p(X, Y)]
$$

**性质 1: 非负性 (Non-negativity)**
联合熵总是非负的：
$$
H(X, Y) \geq 0
$$
*证明思路*: 由于概率 $p(x, y) \in [0, 1]$，因此 $\log_2 p(x, y) \leq 0$。每一项 $-p(x, y) \log_2 p(x, y)$ 都是非负的，故其总和也非负。

#### **2. 条件熵 (Conditional Entropy)**

条件熵衡量在已知一个随机变量 Y 的情况下，另一个随机变量 X 的剩余不确定性。

**定义:** 随机变量 X 在给定随机变量 Y 下的条件熵定义为：
$$
H(X|Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log_2 p(x|y)
$$
它等于所有 Y 取值下 X 的熵的加权平均：
$$
H(X|Y) = \sum_{y \in \mathcal{Y}} p(y) H(X|Y=y)
$$
其中 $H(X|Y=y) = - \sum_{x \in \mathcal{X}} p(x|y) \log_2 p(x|y)$。

#### **3. 熵的链式法则 (Chain Rule for Entropy)**

链式法则将联合熵、边缘熵和条件熵联系在一起。

**定理:**
$$
H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$
*证明思路*: 从联合概率 $p(x, y) = p(x) p(y|x)$ 出发，代入联合熵定义即可推导。

链式法则可以推广到多个随机变量：
$$
H(X_1, X_2, \dots, X_n) = H(X_1) + H(X_2|X_1) + \dots + H(X_n|X_1, \dots, X_{n-1}) = \sum_{i=1}^n H(X_i | X_1, \dots, X_{i-1})
$$

#### **4. 相对熵与互信息 (Relative Entropy and Mutual Information)**

**相对熵 (Relative Entropy)**，也称 KL 散度 (Kullback-Leibler Divergence)，用于衡量两个概率分布 $p$ 和 $q$ 之间的差异性。

**定义:**
$$
D(p||q) = \sum_{x \in \mathcal{X}} p(x) \log_2 \frac{p(x)}{q(x)}
$$

**定理 (信息不等式):** 相对熵总是非负的。
$$
D(p||q) \geq 0
$$
等号成立当且仅当对于所有的 $x$, $p(x) = q(x)$。
*证明思路*: 利用 Jensen 不等式。由于 $-\log_2 x$ 是凸函数，可得：
$$
D(p||q) = - \sum_x p(x) \log_2 \frac{q(x)}{p(x)} \geq - \log_2 \left( \sum_x p(x) \frac{q(x)}{p(x)} \right) = - \log_2 \left( \sum_x q(x) \right) \geq - \log_2 1 = 0
$$

**互信息 (Mutual Information)** 衡量两个随机变量之间共享的信息量。

**定义:**
$$
I(X;Y) = H(X) + H(Y) - H(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
$$
互信息可以表示为联合分布 $p(x,y)$ 与边缘分布乘积 $p(x)p(y)$ 之间的相对熵：
$$
I(X;Y) = D(p(x,y) || p(x)p(y))
$$
根据相对熵的非负性，我们立即得到互信息的非负性：$I(X;Y) \geq 0$。

#### **5. 经典熵的核心性质总结**

1.  **熵的边界:** $0 \leq H(X) \leq \log_2 |\mathcal{X}|$
2.  **次可加性 (Subadditivity):** $H(X, Y) \leq H(X) + H(Y)$
    *   *证明*: $H(X) + H(Y) - H(X,Y) = I(X;Y) \geq 0$。
3.  **条件化降低熵 (Conditioning Reduces Entropy):** $H(X|Y) \leq H(X)$
    *   *证明*: $H(X) - H(X|Y) = I(X;Y) \geq 0$。等号成立当且仅当 X 和 Y 相互独立。
4.  **进一步条件化降低熵:** $H(X|Y, Z) \leq H(X|Y)$
    *   *证明*: 这等价于条件互信息 $I(X;Z|Y) \geq 0$。这表明在已知 Y 的情况下，关于 Z 的额外信息只会减少（或保持不变）X 的不确定性。

好的，这是根据您的笔记和参考教材整理的第二页内容。

***

### **量子信息讲义 - 第 2 页 / 共 13 页**

---

### **第二部分：熵 (Entropy)**

本节将深入探讨熵的几种定义及其基本性质，特别是 Shannon 熵的数学特性。

#### **1. 熵的推广 (Generalizations of Entropy)**

除了 Shannon 熵，信息论中还存在其他几种熵的定义，它们在不同场景下提供了对信息量的不同视角。

*   **Hartley 熵:** 当一个系统有 $N$ 个等可能的状态时，其信息量可以用 Hartley 熵来描述，它衡量了区分这些状态所需的信息。
    $$
    H_0(X) = \log_2 N
    $$
    这可以看作是 Shannon 熵在均匀分布下的特例。

*   **Rényi 熵:** Rényi 熵是 Shannon 熵的一阶参数化推广，定义为：
    $$
    H_\alpha(X) = \frac{1}{1-\alpha} \log_2 \left( \sum_{i=1}^N p_i^\alpha \right)
    $$
    其中 $\alpha \ge 0$ 且 $\alpha \neq 1$。Rényi 熵在不同的 $\alpha$ 值下有特殊的意义：
    *   **当 $\alpha \to 0$ 时:** 趋近于 Hartley 熵，$H_0(X) = \log_2 N$。
    *   **当 $\alpha \to 1$ 时:** 利用洛必达法则，可以证明其极限为 Shannon 熵。
        $$
        \lim_{\alpha \to 1} H_\alpha(X) = H(X)
        $$
        *推导*: 对分子分母同时对 $\alpha$ 求导可得。
    *   **当 $\alpha = 2$ 时:** 称为碰撞熵 (Collision Entropy)。
    *   **当 $\alpha \to \infty$ 时:** 称为最小熵 (Min-Entropy)，$H_\infty(X) = -\log_2 (\max_i p_i)$。

#### **2. 二元熵函数 (Binary Entropy Function)**

二元熵函数是 Shannon 熵在只有两个可能结果的系统（如抛硬币）中的具体表现，是信息论中一个非常重要的函数。

**定义:** 对于一个概率为 $p$ 和 $1-p$ 的二元分布，其熵函数为：
$$
H(p) = -p \log_2 p - (1-p) \log_2 (1-p)
$$

**性质:**
*   **凹性 (Concavity):** 二元熵函数是关于 $p$ 的严格凹函数。这意味着混合不同分布会增加不确定性（熵）。
*   **对称性:** $H(p) = H(1-p)$。
*   **取值范围:** $0 \le H(p) \le 1$。
    *   当 $p=0$ 或 $p=1$ 时，结果是确定的，熵为 $0$。
    *   当 $p=0.5$ 时，不确定性最大，熵达到最大值 $1$ bit。

下图展示了二元熵函数 $H(p)$ 的图像，其形状为一个凸起的弧形。

<p align="center">
  <img src="https://i.imgur.com/8Qj8mB3.png" alt="Binary Entropy Function" width="450"/>
</p>

#### **3. Shannon 熵的基本性质**

1.  **非负性:** $H(X) \ge 0$。等号成立当且仅当分布是确定性的（即只有一个事件的概率为1，其余为0）。

2.  **确定性与对称性:**
    *   对于确定性分布，$p_i = \delta_{ik}$，熵为 $H(X) = 0$。
    *   熵的值仅与概率分布的数值有关，而与事件的排列顺序无关。

3.  **极值性 (Maximum Entropy):** 对于一个有 $d$ 个状态的系统，其熵的最大值为 $\log_2 d$，当且仅当系统服从均匀分布时（即 $p_i = 1/d$ 对所有 $i$ 成立）达到最大值。
    $$
    H(X) \le \log_2 d
    $$
    *证明*:
    **方法一：拉格朗日乘数法**
    我们希望在约束 $\sum_{i=1}^d p_i = 1$ 下最大化 $H(p) = -\sum p_i \log_2 p_i$。构造拉格朗日函数：
    $$
    L(p_1, \dots, p_d, \lambda) = -\sum_{i=1}^d p_i \ln p_i + \lambda \left(\sum_{i=1}^d p_i - 1\right)
    $$
    (为方便求导，使用自然对数 $\ln$)。对 $p_i$ 求偏导并令其为零：
    $$
    \frac{\partial L}{\partial p_i} = -(\ln p_i + 1) + \lambda = 0 \implies \ln p_i = \lambda - 1
    $$
    这表明所有的 $p_i$ 都相等。结合约束 $\sum p_i = 1$，可得 $p_i = 1/d$。此时熵为：
    $$
    H(X) = -\sum_{i=1}^d \frac{1}{d} \log_2 \frac{1}{d} = \log_2 d
    $$
    **方法二：利用相对熵**
    令 $p$ 为任意分布， $u$ 为均匀分布 ($u_i = 1/d$)。根据信息不等式 $D(p||u) \ge 0$：
    $$
    D(p||u) = \sum_i p_i \log_2 \frac{p_i}{u_i} = \sum_i p_i (\log_2 p_i - \log_2 \frac{1}{d}) = -H(p) + \log_2 d
    $$
    因此，$\log_2 d - H(p) \ge 0$，即 $H(p) \le \log_2 d$。等号成立当且仅当 $p=u$。


好的，这是第三页的内容。

***

### 量子信息讲义 - 第 3 页 / 共 13 页

---

### 第三部分：熵与热力学的联系

本节探讨信息熵与物理学中的热力学熵之间的深刻联系，通过一个经典的思想实验——麦克斯韦妖（Maxwell's demon），来揭示信息在物理过程中的作用。

### 1. 熵的物理意义

信息熵和热力学熵在形式上非常相似，这并非巧合。玻尔兹曼 (Boltzmann) 将热力学熵 S 与系统微观状态的数量 W 联系起来：
$$
S = k_B \ln W
$$
其中 $k_B$ 是玻尔兹曼常数。这与 Hartley 熵 $H_0 = \log_2 N$ 形式一致。对于非均匀分布，吉布斯熵 (Gibbs Entropy) 的形式为：
$$
S = -k_B \sum_i p_i \ln p_i
$$
这与 Shannon 熵 $H(p) = -\sum p_i \log_2 p_i$ 几乎完全相同，仅相差一个常数因子 $k_B \ln 2$。

这种相似性表明，信息熵可以被看作是热力学熵在信息领域的体现。一个系统的热力学熵反映了我们对其微观状态的无知程度；而信息熵则量化了我们对一个随机变量取值的无知程度。

### 2. 热力学第二定律与信息

热力学第二定律指出，一个孤立系统的熵永不减少。
$$
\Delta S \ge 0
$$
然而，当我们引入一个能够获取系统微观信息的“智能体”时，情况似乎变得复杂。

*   **等温过程 (Isothermal process):** $\Delta S \ge \frac{Q}{T}$
*   **绝热过程 (Adiabatic process):** $\Delta S \ge 0$
*   **可逆过程 (Reversible process):** $\Delta S = \frac{Q}{T}$
*   **等熵过程 (Isentropic process):** $\Delta S = 0$

这些过程描述了宏观系统在没有外部“智能”干预下的演化。麦克斯韦妖的提出，正是为了挑战热力学第二定律的普适性。

### 3. 麦克斯韦妖 (Maxwell's demon)

1867年，麦克斯韦提出了一个思想实验：一个容器被隔板分成两半，中间有一个小门，由一个“妖”控制。这个“妖”可以观察单个气体分子的速度，并选择性地开门，让快分子都跑到一侧，慢分子都跑到另一侧。

如此一来，系统的一部分温度升高，另一部分温度降低，从而在没有做功的情况下，使系统的总熵减少了。这似乎违背了热力学第二定律。

### 4. 西拉德引擎 (Szilard Engine)

1929年，西拉德 (Szilard) 将麦克斯韦妖思想实验具体化为一个可以从单一热源中提取功的循环引擎——西拉德引擎。

**引擎的循环过程：**
1.  **步骤 1 (Partition):** 将一个装有单个气体分子的容器从中间插入隔板，分子被困在其中一侧。
2.  **步骤 2 (Measurement):** “妖”测量分子在哪一侧。这个测量过程获得了 1 bit 的信息（左或右）。
3.  **步骤 3 (Expansion):** 根据测量的结果，在分子所在的那一侧连接一个活塞。由于分子对活塞的碰撞，气体进行等温膨胀，对外做功。功的大小为 $W = k_B T \ln 2$。
4.  **步骤 4 (Reset):** 移除隔板，系统回到初始状态。

**熵的变化：**
*   **气体:** 在等温膨胀过程中，气体从热源吸收热量 $Q = W = k_B T \ln 2$，其熵增加 $\Delta S_{gas} = \frac{Q}{T} = k_B \ln 2$。
*   **妖 (Demon):** 为了完成一个循环，“妖”必须“忘记”之前测量得到的信息。根据兰道尔原理 (Landauer's principle)，擦除 1 bit 的信息至少需要向环境中耗散 $k_B T \ln 2$ 的能量，这导致环境熵增加至少 $k_B \ln 2$。

**结论：**
整个系统（气体 + 妖 + 环境）的总熵变化 $\Delta S_{total} \ge 0$。麦克斯韦妖并没有真正违背热力学第二定律。信息的获取和擦除是有物理代价的，这个代价恰好弥补了因“智能”操作而减少的熵。

**信息即熵 (Information is Entropy):**
这个思想实验揭示了信息和熵的深刻等价性。1 bit 的信息等价于 $k_B \ln 2$ 的热力学熵。
$$
I \iff S = k_B I \ln 2
$$
获取信息的过程（测量）会降低系统的熵，但为了使“妖”能够持续工作，必须擦除旧的信息，而信息的擦除是一个不可逆的过程，必然导致整个系统的总熵增加。

好的，这是第四页的内容。

***

### 量子信息讲义 - 第 4 页 / 共 13 页

---

### 第四部分：概率论基础回顾

本节将回顾在量子信息论中至关重要的概率论基础概念，特别是基于测度论的严格定义。这将为后续理解条件期望等高级概念奠定基础。

### 1. 核心概念

*   **概率空间 (Probability Space):** 一个概率空间由三元组 $(\Omega, \mathcal{F}, P)$ 构成。
    *   $\Omega$: 样本空间 (Sample Space)，是所有可能结果的集合。
    *   $\mathcal{F}$: 事件空间 (Event Space)，是 $\Omega$ 的一个子集族，构成一个 $\sigma$-代数 (σ-algebra)。它包含了我们关心的所有事件。
    *   $P$: 概率测度 (Probability Measure)，是一个从 $\mathcal{F}$ 到 $[0, 1]$ 的函数，满足可数可加性等公理。

*   **随机变量 (Random Variable):** 一个随机变量 $X$ 是一个从样本空间 $\Omega$ 到某个可测空间（通常是实数集 $\mathbb{R}$）的可测函数。
    *   **可测性 (Measurability):** 对于值域中的任何博雷尔集 $B \in \mathcal{B}(\mathbb{R})$，其在 $X$ 下的原像 $X^{-1}(B) = \{\omega \in \Omega | X(\omega) \in B\}$ 必须是事件空间 $\mathcal{F}$ 中的一个事件。

*   **概率分布 (Probability Distribution):** 随机变量 $X$ 的概率分布 $P_X$ 描述了 $X$ 取不同值的可能性。它由概率测度 $P$ 和随机变量 $X$ 共同决定：
    $$
    P_X(B) = P(X^{-1}(B)) = P(\{\omega \in \Omega | X(\omega) \in B\})
    $$

### 2. 期望 (Expectation)

随机变量的期望或均值是其所有可能取值的加权平均。

**定义 (基于测度论):** 对于一个随机变量 $X: \Omega \to \mathbb{R}$，其期望值定义为它关于概率测度 $P$ 的勒贝格积分 (Lebesgue integral)：
$$
\mathbb{E}[X] = \int_\Omega X(\omega) dP(\omega)
$$
如果 $X$ 的分布由概率密度函数 $f(x)$ 给出，上式可以写为更常见的形式：
$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x f(x) dx
$$
对于离散随机变量，则写为：
$$
\mathbb{E}[X] = \sum_i x_i p(x_i)
$$

### 3. 条件概率 (Conditional Probability)

**初等定义:** 事件 A 在给定事件 B 发生下的条件概率为：
$$
P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad \text{其中 } P(B) > 0
$$

**测度论定义 (Radon-Nikodym 定理):**
给定一个子 $\sigma$-代数 $\mathcal{G} \subseteq \mathcal{F}$，事件 $A \in \mathcal{F}$ 关于 $\mathcal{G}$ 的条件概率是一个 $\mathcal{G}$-可测的随机变量 $P(A|\mathcal{G})$，它满足对任意 $G \in \mathcal{G}$ 都有：
$$
\int_G P(A|\mathcal{G}) dP = P(A \cap G)
$$
这个定义更加普遍，因为它允许条件是基于一个信息集（由 $\sigma$-代数 $\mathcal{G}$ 描述），而不仅仅是单个事件。

### 4. 条件期望 (Conditional Expectation)

条件期望 $\mathbb{E}[X|\mathcal{G}]$ 是对随机变量 $X$ 在给定信息集 $\mathcal{G}$ 下的最佳估计。

**定义:**
给定一个概率空间 $(\Omega, \mathcal{F}, P)$，一个随机变量 $X$，和一个子 $\sigma$-代数 $\mathcal{G} \subseteq \mathcal{F}$，条件期望 $\mathbb{E}[X|\mathcal{G}]$ 是一个满足以下两个条件的随机变量：
1.  **可测性:** $\mathbb{E}[X|\mathcal{G}]$ 是 $\mathcal{G}$-可测的。这意味着它的值完全由 $\mathcal{G}$ 中的信息决定。
2.  **部分平均 (Partial Averaging):** 对于任何事件 $G \in \mathcal{G}$，满足：
    $$
    \int_G \mathbb{E}[X|\mathcal{G}] dP = \int_G X dP
    $$
    这个性质表明，在 $\mathcal{G}$ 中的任何区域上，$X$ 的期望值等于其条件期望的期望值。

**重要性质:**
*   **线性:** $\mathbb{E}[aX+bY|\mathcal{G}] = a\mathbb{E}[X|\mathcal{G}] + b\mathbb{E}[Y|\mathcal{G}]$
*   **塔性质 (Tower Property):** 如果 $\mathcal{H} \subseteq \mathcal{G} \subseteq \mathcal{F}$ 是嵌套的 $\sigma$-代数，则：
    $$
    \mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}] = \mathbb{E}[X|\mathcal{H}]
    $$
    特别地，$\mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X]$。
*   **提取已知信息:** 如果 $Y$ 是 $\mathcal{G}$-可测的，则 $\mathbb{E}[XY|\mathcal{G}] = Y\mathbb{E}[X|\mathcal{G}]$。

好的，这是第五页的内容。

***

### 量子信息讲义 - 第 5 页 / 共 13 页

---

### 第五部分：条件期望的深入理解

本节将通过一个具体的例子来进一步阐明条件期望的抽象定义，并展示其在概率论中的实际计算方法。

### 1. 条件期望作为一种“投影”

从几何角度看，条件期望可以被理解为一种投影操作。在由随机变量构成的希尔伯特空间 $L^2(\Omega, \mathcal{F}, P)$ 中，条件期望 $\mathbb{E}[X|\mathcal{G}]$ 是将随机变量 $X$ 投影到由所有 $\mathcal{G}$-可测随机变量构成的子空间 $L^2(\Omega, \mathcal{G}, P)$ 上的正交投影。

这个投影是“最佳估计”的精确数学含义：在所有 $\mathcal{G}$-可测的随机变量 $Y$ 中，条件期望 $\mathbb{E}[X|\mathcal{G}]$ 是使得均方误差 $\mathbb{E}[(X-Y)^2]$ 最小化的那一个。

### 2. 离散情况下的条件期望

为了更好地理解条件期望的定义，我们来看一个离散的例子。

**问题设置:**
假设我们掷两个公平的四面骰子，样本空间 $\Omega = \{(i, j) | i, j \in \{1, 2, 3, 4\}\}$，共有 16 个等可能的结果，每个结果的概率为 $P(\{(i,j)\}) = 1/16$。
我们定义以下随机变量：
*   $X(\omega) = i+j$：两个骰子点数之和。
*   $Y(\omega) = \max(i, j)$：两个骰子点数的最大值。

我们想计算条件期望 $\mathbb{E}[X | Y]$。

**步骤:**

1.  **确定信息集:** 条件是关于随机变量 $Y$ 的，因此我们关心的子 $\sigma$-代数 $\mathcal{G}$ 是由 $Y$ 生成的 $\sigma$-代数，记为 $\sigma(Y)$。
    $\sigma(Y)$ 由形如 $Y^{-1}(B)$ 的集合生成，其中 $B \subseteq \{1, 2, 3, 4\}$。这意味着 $\sigma(Y)$ 中的事件都是关于 $Y$ 的取值的陈述，例如事件“$Y=3$”（即两个骰子最大值为3）就是一个典型的事件。
    $\sigma(Y)$ 划分了样本空间 $\Omega$：
    *   $A_1 = \{Y=1\} = \{(1,1)\}$
    *   $A_2 = \{Y=2\} = \{(1,2), (2,1), (2,2)\}$
    *   $A_3 = \{Y=3\} = \{(1,3), (2,3), (3,1), (3,2), (3,3)\}$
    *   $A_4 = \{Y=4\} = \{(1,4), (2,4), (3,4), (4,1), (4,2), (4,3), (4,4)\}$
    这个由 $\{A_1, A_2, A_3, A_4\}$ 构成的划分是 $\sigma(Y)$ 的基础。任何 $\sigma(Y)$-可测的随机变量在这些划分的每个集合上都必须取常数值。

2.  **计算条件期望:**
    根据定义，$\mathbb{E}[X|Y]$ 是一个在每个划分 $A_k$ 上取常数值的随机变量。其在 $A_k$ 上的取值，我们记为 $z_k$，应该是在事件 $A_k$ 发生时 $X$ 的期望值。这正是条件期望的初等定义：
    $$
    z_k = \mathbb{E}[X | Y=k] = \sum_x x \cdot P(X=x | Y=k)
    $$
    我们来计算每个 $z_k$：
    *   **对于 $A_1 = \{Y=1\}$:**
        该集合中只有一个元素 $(1,1)$，此时 $X=2$。
        $\mathbb{E}[X|Y=1] = 2$。
    *   **对于 $A_2 = \{Y=2\}$:**
        该集合中有三个元素 $\{(1,2), (2,1), (2,2)\}$，它们的 $X$ 值分别为 $3, 3, 4$。
        $\mathbb{E}[X|Y=2] = \frac{3+3+4}{3} = \frac{10}{3}$。
    *   **对于 $A_3 = \{Y=3\}$:**
        该集合中有五个元素，它们的 $X$ 值分别为 $4, 5, 4, 5, 6$。
        $\mathbb{E}[X|Y=3] = \frac{4+5+4+5+6}{5} = \frac{24}{5}$。
    *   **对于 $A_4 = \{Y=4\}$:**
        该集合中有七个元素，它们的 $X$ 值分别为 $5, 6, 7, 5, 6, 7, 8$。
        $\mathbb{E}[X|Y=4] = \frac{5+6+7+5+6+7+8}{7} = \frac{44}{7}$。

3.  **表示为随机变量:**
    因此，条件期望 $\mathbb{E}[X|Y]$ 是一个随机变量，其取值依赖于 $Y$ 的值：
    $$
    \mathbb{E}[X|Y](\omega) =
    \begin{cases}
    2 & \text{if } Y(\omega) = 1 \\
    10/3 & \text{if } Y(\omega) = 2 \\
    24/5 & \text{if } Y(\omega) = 3 \\
    44/7 & \text{if } Y(\omega) = 4
    \end{cases}
    $$

**验证测度论定义:**
我们验证一下 $G = A_2 = \{Y=2\}$ 的情况。$P(A_2) = 3/16$。
*   左边：$\int_{A_2} \mathbb{E}[X|Y] dP = \mathbb{E}[X|Y=2] \cdot P(A_2) = \frac{10}{3} \cdot \frac{3}{16} = \frac{10}{16}$。
*   右边：$\int_{A_2} X dP = \sum_{\omega \in A_2} X(\omega) P(\{\omega\}) = (3+3+4) \cdot \frac{1}{16} = \frac{10}{16}$。
两者相等，符合定义。

这个例子清楚地表明，条件期望 $\mathbb{E}[X|Y]$ 是一个函数，它的自变量是样本点 $\omega \in \Omega$，但它的值仅通过 $Y(\omega)$ 决定。它将 $X$ 在给定信息 $Y$ 下的“局部平均值”进行了编码。


好的，这是第六页的内容。

***

### 量子信息讲义 - 第 6 页 / 共 13 页

---

### 第六部分：重要公式与概念汇总

本页总结了经典和量子信息论中的核心公式和概念，作为快速参考和复习的工具。

### 1. 经典信息论 (Classical Information Theory)

*   **信息量 (Self-Information):**
    $I(p) = -\log_2 p$

*   **Shannon 熵 (Shannon Entropy):**
    $H(X) = -\sum_i p_i \log_2 p_i$

*   **联合熵 (Joint Entropy):**
    $H(X, Y) = -\sum_{i,j} p(x_i, y_j) \log_2 p(x_i, y_j)$

*   **条件熵 (Conditional Entropy):**
    $H(X|Y) = H(X,Y) - H(Y)$

*   **相对熵 (Relative Entropy / KL Divergence):**
    $D(p||q) = \sum_i p_i \log_2 \frac{p_i}{q_i}$

*   **互信息 (Mutual Information):**
    $I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = D(p(x,y) || p(x)p(y))$

*   **熵的链式法则 (Chain Rule):**
    $H(X_1, \dots, X_n) = \sum_{i=1}^n H(X_i | X_1, \dots, X_{i-1})$
    $I(X_1, \dots, X_n ; Y) = \sum_{i=1}^n I(X_i ; Y | X_1, \dots, X_{i-1})$

*   **Rényi 熵:**
    $H_\alpha(X) = \frac{1}{1-\alpha} \log_2 \left( \sum_i p_i^\alpha \right)$
    *   $\alpha=0$: Hartley 熵
    *   $\alpha \to 1$: Shannon 熵
    *   $\alpha=2$: 碰撞熵
    *   $\alpha \to \infty$: 最小熵

### 2. 量子信息论 (Quantum Information Theory)

以下公式是经典概念在量子力学框架下的推广。

*   **冯·诺依曼熵 (Von Neumann Entropy):**
    对于密度矩阵 $\rho$，其冯·诺依曼熵定义为：
    $$
    S(\rho) = -\text{Tr}(\rho \log_2 \rho) = -\sum_i \lambda_i \log_2 \lambda_i
    $$
    其中 $\lambda_i$ 是 $\rho$ 的特征值。
    *   **特性:** 凹性 (Concavity)

*   **量子联合熵 (Quantum Joint Entropy):**
    对于复合系统的密度矩阵 $\rho_{AB}$：
    $S(A, B) = S(\rho_{AB}) = -\text{Tr}(\rho_{AB} \log_2 \rho_{AB})$

*   **量子条件熵 (Quantum Conditional Entropy):**
    $S(A|B) = S(A, B) - S(B)$
    *   **特性:** 与经典条件熵不同，量子条件熵可以为负。

*   **量子相对熵 (Quantum Relative Entropy):**
    $D(\rho||\sigma) = \text{Tr}(\rho (\log_2 \rho - \log_2 \sigma))$
    *   **特性:** 非负性 (Klein's Inequality)，联合凸性 (Joint Convexity)

*   **量子互信息 (Quantum Mutual Information):**
    $I(A;B) = S(A) + S(B) - S(A, B)$
    *   **特性:** 非负性， $I(A;B) = D(\rho_{AB} || \rho_A \otimes \rho_B)$

*   **相干信息 (Coherent Information):**
    $I(A \rangle B)_\rho = S(B) - S(A,B) = -S(A|B)$
    *   **用途:** 与量子信道容量密切相关。

*   **保真度 (Fidelity):**
    $F(\rho, \sigma) = \left( \text{Tr} \sqrt{\sqrt{\rho}\sigma\sqrt{\rho}} \right)^2$
    *   **对于纯态:** $F(|\psi\rangle\langle\psi|, |\phi\rangle\langle\phi|) = |\langle\psi|\phi\rangle|^2$

### 3. 数据处理不等式 (Data Processing Inequality)

*   **经典:** 对于马尔可夫链 $X \to Y \to Z$：
    $I(X;Y) \ge I(X;Z)$

*   **量子:** 对于任意量子信道 $\mathcal{E}$：
    $D(\rho||\sigma) \ge D(\mathcal{E}(\rho)||\mathcal{E}(\sigma))$
    这个不等式（量子相对熵的单调性）是量子信息论中许多其他重要不等式的基础，包括强次可加性。

---
**重要提示:** 量子信息论中的许多概念虽然形式上与经典对应物相似，但其物理和数学内涵有本质区别。例如，量子条件熵可以为负，这反映了量子纠缠的独特性质。

好的，这是第七页的内容。

***

### 量子信息讲义 - 第 7 页 / 共 13 页

---

### 第七部分：相对熵的性质

本节将深入探讨经典相对熵和量子相对熵的一些关键数学性质，这些性质在信息论的证明和应用中至关重要。

### 1. 经典相对熵 (Kullback-Leibler Divergence)

**a) 联合凸性 (Joint Convexity)**
相对熵 $D(p||q)$ 是关于概率分布对 $(p, q)$ 的联合凸函数。也就是说，对于任意两对概率分布 $(p_1, q_1)$ 和 $(p_2, q_2)$，以及任意 $\lambda \in [0, 1]$，有：
$$
D(\lambda p_1 + (1-\lambda)p_2 || \lambda q_1 + (1-\lambda)q_2) \leq \lambda D(p_1||q_1) + (1-\lambda)D(p_2||q_2)
$$
*证明思路*:
利用对数求和不等式 (log sum inequality) 可以证明。对数求和不等式指出，对于非负数 $a_1, \dots, a_n$ 和 $b_1, \dots, b_n$：
$$
\sum_{i=1}^n a_i \log \frac{a_i}{b_i} \ge \left(\sum_{i=1}^n a_i\right) \log \frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i}
$$
将 $a_i = \lambda p_1(i) + (1-\lambda)p_2(i)$ 和 $b_i = \lambda q_1(i) + (1-\lambda)q_2(i)$ 代入并进行变换即可。

**b) 数据处理不等式 (Data Processing Inequality)**
[[数据处理不等式\|数据处理不等式]] 
如果随机变量构成马尔可夫链 $X \to Y \to Z$（即给定 Y 时，X 和 Z 条件独立），则有：
$$
I(X;Y) \ge I(X;Z)
$$
这个不等式直观地说明，信息在处理过程中只会丢失或保持不变，不会增加。
它等价于相对熵在随机映射（信道）下的单调性：
$$
D(p(x)||q(x)) \ge D(p(y)||q(y))
$$
其中 $p(y)$ 和 $q(y)$ 是通过同一个条件概率 $p(y|x)$ 从 $p(x)$ 和 $q(x)$ 得到的。

### 2. 量子相对熵 (Quantum Relative Entropy)

量子相对熵继承并推广了经典相对熵的许多重要性质。

**a) Klein 不等式 (Klein's Inequality):**
对于任意两个密度矩阵 $\rho$ 和 $\sigma$，量子相对熵是非负的：
$$
D(\rho||\sigma) \ge 0
$$
等号成立当且仅当 $\rho = \sigma$。

**b) Lieb 凹性定理 (Lieb's Concavity Theorem) 与联合凸性 (Joint Convexity)**
Lieb 证明了一个关于算子函数的强凹性定理，其一个重要推论是量子相对熵 $D(\rho||\sigma)$ 是关于 $(\rho, \sigma)$ 的联合凸函数。
$$
D(\lambda \rho_1 + (1-\lambda)\rho_2 || \lambda \sigma_1 + (1-\lambda)\sigma_2) \leq \lambda D(\rho_1||\sigma_1) + (1-\lambda)D(\rho_2||\sigma_2)
$$
这是量子信息论中最深刻和最有力的结果之一。

**c) 量子数据处理不等式 (Monotonicity under CPTP maps)**
对于任意完全正定保迹 (CPTP) 映射 $\mathcal{E}$（即量子信道），量子相对熵是单调不增的：
$$
D(\rho||\sigma) \ge D(\mathcal{E}(\rho)||\mathcal{E}(\sigma))
$$
*证明思路*: 该不等式是量子相对熵联合凸性的直接推论。通过构造一个特殊的块矩阵并应用联合凸性可以证明。

### 3. Pinsker 不等式 (Pinsker's Inequality)

Pinsker 不等式建立了相对熵和迹距离 (trace distance) 之间的定量关系，为信息论距离和度量距离提供了桥梁。

**经典 Pinsker 不等式:**
$$
D(p||q) \ge \frac{1}{2 \ln 2} \|p-q\|_1^2
$$
其中 $\|p-q\|_1 = \sum_i |p_i - q_i|$ 是一范数距离。

**量子 Pinsker 不等式:**
$$
D(\rho||\sigma) \ge \frac{1}{2 \ln 2} \|\rho-\sigma\|_1^2
$$
其中 $\|\rho-\sigma\|_1 = \text{Tr}|\rho-\sigma|$ 是迹距离。这个不等式表明，如果两个量子态的相对熵很小，那么它们在迹距离的意义下也必定很接近。


好的，这是第八页的内容。

***

### 量子信息讲义 - 第 8 页 / 共 13 页

---

### 第八部分：量子信道与数据处理

本节讨论量子信道（Quantum Channel）如何处理量子态，并探讨信息在这一过程中的变化。核心结论是，量子操作（信道）永远不会增加系统间的可区分性或信息量。

### 1. 量子数据处理不等式

量子数据处理不等式是量子信息论的基石之一。它表明，对两个量子态施加任何物理上允许的操作（即量子信道），它们之间的可区分性（用相对熵衡量）都不会增加。

**定理 (量子相对熵的单调性):**
对于任意量子信道 $\mathcal{E}$（一个CPTP映射）和任意两个密度矩阵 $\rho, \sigma$，有：
$$
D(\rho || \sigma) \ge D(\mathcal{E}(\rho) || \mathcal{E}(\sigma))
$$

*证明思路（利用 Stinespring Dilation）:*
任何量子信道 $\mathcal{E}: \mathcal{H}_A \to \mathcal{H}_B$ 都可以通过一个辅助系统（环境）$\mathcal{H}_E$ 上的酉演化 $U$ 来实现，即 Stinespring 表示：
$$
\mathcal{E}(\rho) = \text{Tr}_E [U(\rho \otimes |0\rangle\langle0|_E)U^\dagger]
$$
其中 $|0\rangle_E$ 是环境的初始状态。
1.  首先，将 $\rho$ 和 $\sigma$ 扩展到包含环境的复合系统：$\rho \otimes |0\rangle\langle0|_E$ 和 $\sigma \otimes |0\rangle\langle0|_E$。由于张量积的性质，相对熵保持不变：
    $D(\rho \otimes |0\rangle\langle0|_E || \sigma \otimes |0\rangle\langle0|_E) = D(\rho||\sigma)$。
2.  酉演化 $U$ 不改变相对熵，因为 $D(U\rho U^\dagger || U\sigma U^\dagger) = D(\rho||\sigma)$。
3.  最后，对复合系统取偏迹（tracing out the environment）是一个CPTP映射。任何偏迹操作都会导致相对熵减少或不变。
    因此， $D(U(\rho \otimes |0\rangle\langle0|_E)U^\dagger || U(\sigma \otimes |0\rangle\langle0|_E)U^\dagger) \ge D(\text{Tr}_E[U(\rho \otimes |0\rangle\langle0|_E)U^\dagger] || \text{Tr}_E[U(\sigma \otimes |0\rangle\langle0|_E)U^\dagger])$。
结合以上步骤，即可得证。

### 2. 数据处理不等式的推论

量子数据处理不等式有许多重要的推论，它们构成了量子信息论中关于信息流动的核心法则。

**a) 互信息的单调性**
对于一个复合系统 $\rho_{AB}$，对其子系统 B 施加一个量子信道 $\mathcal{E}_B$，互信息不会增加：
$$
I(A;B)_\rho \ge I(A;B)_{(\mathcal{I}_A \otimes \mathcal{E}_B)(\rho)}
$$
其中 $\mathcal{I}_A$ 是作用在子系统 A 上的恒等信道。
*直观理解:* 对系统的一部分进行局部操作（即使是可逆的酉操作），也无法增加它与另一部分之间的总关联。信息一旦通过局部操作被“扰乱”，就很难完全恢复。

**b) 量子 Fano 不等式 (Quantum Fano Inequality)**
这个不等式为从量子态恢复经典信息的错误率提供了一个下界。假设我们有一个信道，试图传输一个经典随机变量 $X$，编码到量子态 $\rho_x$ 中。解码过程是一个量子测量。量子 Fano 不等式将解码的错误概率 $p_e$ 与信道输出和经典信息之间的 Holevo 信息联系起来。

### 3. Holevo 信息与 Holevo 界

Holevo 信息 $\chi$ 是一个关键量，它为通过量子信道传输经典信息的能力提供了一个上界。

**定义 (Holevo 信息):**
给定一个由概率 $\{p_x\}$ 和对应量子态 $\{\rho_x\}$ 构成的系综，其 Holevo 信息定义为：
$$
\chi(\{p_x, \rho_x\}) = S\left(\sum_x p_x \rho_x\right) - \sum_x p_x S(\rho_x)
$$
它衡量了系综的平均态的熵与各个态的平均熵之差。

**定理 (Holevo's Bound):**
通过一个量子信道，一次使用能够可靠传输的经典信息量（以互信息 $I(X;Y)$ 度量）不会超过该信道所能承载的最大 Holevo 信息。
$$
I(X;Y) \le \chi
$$
这个界限是量子信道经典容量理论的出发点。它表明，即使你用非正交的量子态来编码信息，你也无法“塞进”比 Holevo 信息更多的经典比特。

**推论:** 发送 $n$ 个量子比特最多只能可靠地传输 $n$ 个经典比特的信息（当且仅当使用正交态编码时）。这是因为对于一个 $d$ 维系统，$\chi \le S(\rho) \le \log_2 d$。对于 $n$ 个量子比特，系统维度为 $2^n$，所以 $I(X;Y) \le n$。


好的，这是第九页的内容。

***

### 量子信息讲义 - 第 9 页 / 共 13 页

---

### 第九部分：强次可加性及其应用

强次可加性（Strong Subadditivity, SSA）是冯·诺依曼熵最重要的性质之一，也是量子信息论中最深刻的不等式之一。它揭示了量子系统中信息关联的复杂结构。

### 1. 强次可加性 (Strong Subadditivity)

**定理 (Lieb-Ruskai, 1973):**
对于任意一个由三部分组成的复合量子系统 ABC，其状态为 $\rho_{ABC}$，冯·诺依曼熵满足以下不等式：
$$
S(A, B, C) + S(B) \le S(A, B) + S(B, C)
$$

这个不等式有多种等价形式，通过重新排列和使用条件熵的定义，可以得到更直观的表述。

**等价形式 1 (条件熵的单调性):**
$$
S(A|B, C) \le S(A|B)
$$
*直观理解:* 这个形式最能体现 SSA 的核心思想。它表明，对条件系统增加一部分 (C) 不会增加我们对目标系统 (A) 的不确定性。换句话说，更多的信息（关于C）只会减少或保持我们对 A 的无知程度。这与经典熵的性质 $H(X|Y,Z) \le H(X|Y)$ 完全一致。

**等价形式 2 (互信息的单调性):**
$$
I(A;B) \le I(A;B, C)
$$
*直观理解:* 系统 A 和系统 B 的互信息，不会超过系统 A 与一个更大的系统 BC 的互信息。这表明，对其中一个系统 B 附加另一个系统 C，不会减少它与 A 之间的总信息关联。

**等价形式 3 (条件互信息非负性):**
$$
I(A;C|B) = S(A|B) - S(A|B,C) \ge 0
$$
这个量被称为条件量子互信息，它衡量了在已知 B 的情况下，A 和 C 之间共享的信息量。强次可加性保证了这个量总是非负的。

*证明思路*:
SSA 的原始证明非常复杂，依赖于 Lieb 凹性定理。一个更现代且直观的证明方法是利用量子相对熵的单调性。考虑复合系统 $\rho_{ABC}$ 和它的边缘态 $\rho_A \otimes \rho_{BC}$，对它们作用一个偏迹信道 $\text{Tr}_A$：
$$
D(\rho_{ABC} || \rho_A \otimes \rho_{BC}) \ge D(\text{Tr}_A(\rho_{ABC}) || \text{Tr}_A(\rho_A \otimes \rho_{BC}))
$$
展开相对熵的定义并化简，即可得到 SSA 的一种形式。

### 2. 强次可加性的应用

强次可加性是许多量子信息论核心定理的基石。

**a) 数据处理不等式的证明**
量子数据处理不等式 $I(A;C) \le I(A;B)$ 对于马尔可夫链 $A \to B \to C$ 成立，这个结论本身就可以通过 SSA 来证明。

**b) 量子信道容量**
在计算量子信道的容量时，尤其是在证明信道编码定理的正向部分（achievability）和反向部分（converse）时，SSA 是不可或缺的工具。例如，它被用来证明 Holevo-Schumacher-Westmoreland (HSW) 定理中经典容量的表达式。

**c) 纠缠理论**
SSA 在量化和理解量子纠缠方面扮演着核心角色。例如，它被用来证明纠缠的度量（如纠缠形成熵）在局部操作和经典通信（LOCC）下是单调的，这是任何合理的纠缠度量都必须满足的基本要求。

**d) 纠缠辅助通信**
在研究利用预共享纠缠来增强信道容量的场景中，SSA 也是分析信息交换和资源消耗的关键。它帮助确定了在有纠缠辅助的情况下，经典通信容量的精确表达式。


好的，这是第十页的内容。

***

### 量子信息讲愈 - 第 10 页 / 共 13 页

---

### 第十部分：量子熵的核心性质

本节将系统地总结和证明冯·诺依曼熵 (Von Neumann Entropy) 的一系列核心性质。这些性质不仅是其作为信息度量的理论基础，也广泛应用于量子计算和量子通信的分析中。

### 1. 基本性质

**a) 纯态熵为零**
如果一个量子系统处于纯态 $|\psi\rangle$，其密度矩阵 $\rho = |\psi\rangle\langle\psi|$ 只有一个非零特征值 1。因此，其熵为：
$$
S(\rho) = -1 \log_2 1 = 0
$$
这与经典情况一致，一个确定状态的系统不含有任何不确定性。

**b) 均匀混合态的熵最大**
对于一个 $d$ 维希尔伯特空间，熵最大的态是完全混合态 $\rho = \frac{1}{d} \mathbb{I}$，其所有特征值均为 $1/d$。最大熵为：
$$
S\left(\frac{1}{d}\mathbb{I}\right) = -\sum_{i=1}^d \frac{1}{d} \log_2 \frac{1}{d} = \log_2 d
$$
这表明，当我们对系统的状态一无所知时，不确定性达到顶峰。

**c) 酉不变性 (Unitary Invariance)**
在酉演化下，系统的熵保持不变。
$$
S(U\rho U^\dagger) = S(\rho)
$$
*证明*: $U\rho U^\dagger$ 和 $\rho$ 具有相同的特征值谱，因此它们的熵相同。这表明可逆的量子动力学过程不产生也不消灭信息。

**d) 凹性 (Concavity)**
冯·诺依曼熵是密度矩阵的凹函数。对于任意两个密度矩阵 $\rho_1, \rho_2$ 和 $\lambda \in [0, 1]$：
$$
S(\lambda \rho_1 + (1-\lambda)\rho_2) \ge \lambda S(\rho_1) + (1-\lambda)S(\rho_2)
$$
*证明思路*: 这是强次可加性的一个推论。考虑一个辅助的经典系统来标记状态的来源，然后应用 SSA。
*直观理解*: 混合不同的量子态会增加系统的不确定性。例如，将两个不同的纯态混合会得到一个混合态，其熵大于零。

### 2. 复合系统的熵性质

**a) 子系统熵 (Entropy of Subsystems)**
对于复合系统 $\rho_{AB}$，其子系统 $\rho_A = \text{Tr}_B(\rho_{AB})$ 和 $\rho_B = \text{Tr}_A(\rho_{AB})$ 的熵满足：
$$
S(A) \le S(A, B) \quad \text{和} \quad S(B) \le S(A, B)
$$
*证明*: 这等价于条件熵 $S(B|A) = S(A,B) - S(A) \ge 0$。然而，**这个经典结论在量子世界中不成立**。

**b) 三角不等式 (Araki-Lieb Triangle Inequality)**
对于复合系统 $\rho_{AB}$，熵满足：
$$
|S(A) - S(B)| \le S(A, B) \le S(A) + S(B)
$$
*   **右边不等式:** $S(A, B) \le S(A) + S(B)$ 称为**次可加性 (Subadditivity)**。它表明整体的不确定性不超过部分不确定性之和。
*   **左边不等式:** $|S(A) - S(B)| \le S(A, B)$ 是一个纯粹的量子效应。它意味着一个子系统的熵可以比整个系统的熵还要大（$S(A) > S(A,B)$），这在经典世界是不可能的。这种情况发生在子系统 A 和 B 之间存在量子纠缠时。

**c) 纯态子系统的熵**
如果复合系统 ABC 处于一个纯态 $|\psi\rangle_{ABC}$，那么任意两个互补子系统的熵相等。例如：
$$
S(A) = S(B,C)
$$
$$
S(A,B) = S(C)
$$
*证明*: 纯态的任何子系统的施密特系数谱 (Schmidt coefficients) 决定了其熵。对于 $|\psi\rangle_{ABC}$，将其看作是 A 和 BC 两个子系统的复合，它们的约化密度矩阵 $\rho_A$ 和 $\rho_{BC}$ 具有相同的非零特征值谱，因此熵相等。

**d) 测量增加熵 (Measurement Increases Entropy)**
对一个量子系统进行投影测量（projective measurement）会增加其平均熵。假设对系统 $\rho$ 进行一组投影测量 $\{\Pi_i\}$，测量后系统塌缩到状态 $\rho_i = \frac{\Pi_i \rho \Pi_i}{\text{Tr}(\Pi_i \rho)}$ 的概率为 $p_i = \text{Tr}(\Pi_i \rho)$。测量后的平均熵为 $\sum_i p_i S(\rho_i)$。我们有：
$$
S(\rho) \le \sum_i p_i S(\rho_i)
$$
这个性质反映了测量过程的不可逆性以及信息从量子态到经典测量结果的转换过程中引入的额外不确定性。

好的，这是第十一页的内容。

***

### 量子信息讲义 - 第 11 页 / 共 13 页

---

### 第十一部分：量子态的纠缠与可分性

本节将介绍量子力学中最引人入胜的概念之一——量子纠缠 (Entanglement)。我们将从数学上严格定义什么是纠缠态和可分态，并探讨它们的基本性质。

### 1. 可分态 (Separable States)

一个复合量子系统的状态如果是可分的，意味着它的各个子系统之间只存在经典关联，可以通过局部操作和经典通信 (LOCC) 来制备。

**定义 (可分纯态):**
对于一个由子系统 A 和 B 组成的复合系统，如果其纯态 $|\psi\rangle_{AB}$ 可以写成两个子系统纯态的张量积形式，则称其为可分纯态或乘积态 (product state)：
$$
|\psi\rangle_{AB} = |\phi\rangle_A \otimes |\chi\rangle_B
$$
对于乘积态，对子系统 A 的测量结果与子系统 B 没有任何关联。

**定义 (可分混合态):**
一个混合态 $\rho_{AB}$ 如果可以表示为一系列乘积态的凸组合（经典概率混合），则称其为可分态 (separable state)：
$$
\rho_{AB} = \sum_i p_i \rho_A^{(i)} \otimes \rho_B^{(i)}
$$
其中 $p_i \ge 0$ 且 $\sum_i p_i = 1$，$\rho_A^{(i)}$ 和 $\rho_B^{(i)}$ 分别是子系统 A 和 B 上的密度矩阵。
*直观理解:* 制备一个可分态的过程是：以概率 $p_i$ 选择制备一对状态 $(\rho_A^{(i)}, \rho_B^{(i)})$，然后在 A 系统上制备 $\rho_A^{(i)}$，在 B 系统上制备 $\rho_B^{(i)}$。这里的关联完全是经典的，源于选择制备哪一对状态的概率过程。

### 2. 纠缠态 (Entangled States)

**定义:**
一个不能表示为可分态形式的复合系统状态，被称为纠缠态 (entangled state)。
$$
\rho_{AB} \text{ is entangled } \iff \rho_{AB} \text{ is not separable}
$$

纠缠态蕴含着非经典的关联。即使两个子系统在空间上相隔很远，对其中一个子系统的测量结果也会瞬间影响到另一个子系统的状态，这种现象被爱因斯坦称为“鬼魅般的超距作用”(spooky action at a distance)。

**例子 (贝尔态 - Bell States):**
最著名的纠缠态例子是两个量子比特系统中的贝尔态。例如：
$$
|\Phi^+\rangle = \frac{1}{\sqrt{2}} (|00\rangle + |11\rangle)
$$
这个态无法写成 $|\phi\rangle_A \otimes |\chi\rangle_B$ 的形式。它的密度矩阵 $\rho = |\Phi^+\rangle\langle\Phi^+|$ 也无法写成可分混合态的凸组合。
其约化密度矩阵为：
$$
\rho_A = \text{Tr}_B(|\Phi^+\rangle\langle\Phi^+|) = \frac{1}{2}\mathbb{I}
$$
$$
\rho_B = \text{Tr}_A(|\Phi^+\rangle\langle\Phi^+|) = \frac{1}{2}\mathbb{I}
$$
子系统本身处于完全混合态（熵最大），但整个复合系统处于纯态（熵为零）。这正是纠缠的标志：
$$
S(A) = S(B) = 1, \quad S(A,B) = 0
$$
$$
I(A;B) = S(A) + S(B) - S(A,B) = 1 + 1 - 0 = 2
$$
其互信息达到了最大可能值，远大于经典比特所能达到的 1 bit。

### 3. 纠缠的判据 (Entanglement Criteria)

判断一个给定的混合态是否纠缠通常是一个困难的问题（NP-hard 问题）。但存在一些必要条件或针对特定情况的充分必要条件。

**a) 偏置换 (Partial Transpose) 与 PPT 判据**
Peres-Horodecki 判据，也称为 PPT (Positive Partial Transpose) 判据。
**操作:** 对密度矩阵 $\rho_{AB}$ 的其中一个子系统（例如 B）做转置操作，得到 $\rho_{AB}^{T_B}$。
$$
(\rho_{AB}^{T_B})_{ij,kl} = (\rho_{AB})_{il,kj}
$$
**定理:**
*   如果一个态 $\rho_{AB}$ 是**可分的**，那么它的偏置换矩阵 $\rho_{AB}^{T_B}$ 必然是**半正定的** (positive semidefinite)。
*   因此，如果 $\rho_{AB}^{T_B}$ 有**负特征值**，则该态**必然是纠缠的**。

**重要说明:**
*   对于 $2 \times 2$ 和 $2 \times 3$ 维的系统，PPT 判据是**充分必要**的。即，一个态是可分的当且仅当其偏置换是半正定的。
*   对于更高维度的系统，PPT 判据只是一个**必要条件**。存在一些纠缠态，它们的偏置换仍然是半正定的，这类态被称为**束缚纠缠** (bound entanglement)。

### 4. 施密特分解 (Schmidt Decomposition)

施密特分解是分析二体纯态纠缠性质的有力工具。

**定理:**
对于任意二体纯态 $|\psi\rangle_{AB} \in \mathcal{H}_A \otimes \mathcal{H}_B$，总能找到 $\mathcal{H}_A$ 的一组标准正交基 $\{|i_A\rangle\}$ 和 $\mathcal{H}_B$ 的一组标准正交基 $\{|i_B\rangle\}$，使得：
$$
|\psi\rangle_{AB} = \sum_{i=1}^d \sqrt{\lambda_i} |i_A\rangle \otimes |i_B\rangle
$$
其中 $d = \min(\dim\mathcal{H}_A, \dim\mathcal{H}_B)$，$\lambda_i \ge 0$ 且 $\sum_i \lambda_i = 1$。
*   $\sqrt{\lambda_i}$ 被称为**施密特系数 (Schmidt coefficients)**。
*   $d$ 的值被称为**施密特秩 (Schmidt rank)**。

**施密特分解与纠缠:**
*   一个纯态是**可分态**当且仅当其**施密特秩为 1**。
*   施密特系数的分布反映了纠缠的程度。系数分布越均匀，纠缠程度越高。当所有非零的 $\lambda_i$ 都相等时，称为**最大纠缠态 (maximally entangled state)**。
*   子系统 $\rho_A$ 和 $\rho_B$ 的非零特征值谱是相同的，都等于 $\{\lambda_i\}$。因此它们的冯·诺依曼熵相等，都等于 $S(\{\lambda_i\})$。这个熵值也常被用作纯态纠缠的度量，称为**纠缠熵 (Entanglement Entropy)**。

好的，这是第十二页的内容。

***

### 量子信息讲义 - 第 12 页 / 共 13 页

---

### 第十二部分：量子互信息与纠缠

本节将深入探讨量子互信息 $I(A;B)$ 的性质，并阐明它与经典关联和量子纠缠之间的关系。虽然量子互信息量化了系统 A 和 B 之间的总关联，但它本身并不能完全等同于纠缠。

### 1. 量子互信息的定义与性质

**回顾定义:**
对于一个二体系统 $\rho_{AB}$，量子互信息定义为：
$$
I(A;B)_\rho = S(A)_\rho + S(B)_\rho - S(A,B)_\rho
$$
其中 $S(A)_\rho = S(\text{Tr}_B \rho_{AB})$，$S(B)_\rho = S(\text{Tr}_A \rho_{AB})$。

**核心性质:**

*   **非负性:** $I(A;B) \ge 0$。这源于相对熵的非负性 $I(A;B) = D(\rho_{AB} || \rho_A \otimes \rho_B)$。
*   **总关联的度量:** 互信息量化了系统 A 和 B 之间总的关联强度。如果 $I(A;B) = 0$，当且仅当 $\rho_{AB} = \rho_A \otimes \rho_B$，即 A 和 B 之间没有任何关联。
*   **局部酉不变性:** 互信息在局部酉操作下保持不变：
    $$
    I(A;B)_\rho = I(A;B)_{(U_A \otimes U_B)\rho(U_A \otimes U_B)^\dagger}
    $$

### 2. 互信息、经典关联与量子纠缠

量子互信息包含两种不同类型的关联：

1.  **经典关联 (Classical Correlations):** 即使是可分态，也可能因为制备过程中的经典随机性而存在关联。例如，状态 $\rho = \frac{1}{2}|00\rangle\langle00| + \frac{1}{2}|11\rangle\langle11|$ 是可分的，但 A 和 B 的测量结果是完全相关的。
2.  **量子关联 (Quantum Correlations) / 纠缠:** 源于非经典的量子叠加，是纠缠态所特有的。

因此，一个重要的问题是：如何将互信息分解为经典和量子两个部分？

**不和谐 (Discord):**
一个常用的方法是定义**量子不和谐 (Quantum Discord)**。经典互信息的一种等价定义是：
$$
I(A;B) = H(A) - H(A|B)
$$
在量子领域，条件熵 $S(A|B)$ 的定义不唯一，因为它依赖于对 B 进行何种测量。一种定义量子互信息的方法是：
$$
\mathcal{J}(A;B)_\rho = S(A) - \min_{\{\Pi_i^B\}} S(A|\{\Pi_i^B\})
$$
其中，最小化过程遍历了所有对子系统 B 的可能测量（POVM）。$S(A|\{\Pi_i^B\})$ 是测量 B 之后 A 的条件熵的平均值。
$\mathcal{J}(A;B)$ 被认为是 $\rho_{AB}$ 中**经典关联**的总量。

**定义 (量子不和谐):**
量子不和谐被定义为总关联（互信息）与经典关联之差：
$$
\mathcal{D}(A;B)_\rho = I(A;B)_\rho - \mathcal{J}(A;B)_\rho
$$
*   对于可分态，存在一个测量可以完全揭示 A 和 B 的关联而不干扰 A，此时 $I(A;B) = \mathcal{J}(A;B)$，因此不和谐为零。
*   对于纠缠态，任何对 B 的局部测量通常都会不可避免地干扰整个系统（即使测量结果未知），导致无法完全提取 A 的信息。因此 $I(A;B) > \mathcal{J}(A;B)$，不和谐大于零。

**重要结论:**
*   一个态是可分的 $\implies$ 不和谐为零。
*   不和谐为零 $\not\implies$ 态是可分的。存在一些被称为“经典-量子”态的特殊可分态，其不和谐为零。但在大多数情况下，我们可以认为**不和谐为零是“经典性”的一个标志**。
*   **存在零不和谐但非经典的态**：这是一个微妙的点，显示了量子关联的复杂性。

### 3. Holevo 信息与互信息

Holevo 信息 $\chi$ 和互信息 $I$ 之间存在紧密联系，这在信道容量的讨论中至关重要。

考虑一个场景，Alice 从一个集合 $\{p_x, \rho_x\}$ 中选择一个状态 $\rho_x$ 发送给 Bob。这个过程可以被描述为一个复合的“经典-量子”态：
$$
\rho_{XB} = \sum_x p_x |x\rangle\langle x|_X \otimes \rho_x^B
$$
其中 $\{|x\rangle\}$ 是一组正交的经典基矢。
在这种状态下，可以证明**Holevo 信息等于互信息**：
$$
\chi(\{p_x, \rho_x\}) = I(X;B)_\rho
$$
*证明*:
$I(X;B) = S(X) + S(B) - S(X,B)$
$S(X) = H(\{p_x\})$
$S(B) = S(\sum_x p_x \rho_x)$
$S(X,B) = S(\sum_x p_x |x\rangle\langle x| \otimes \rho_x) = -\sum_x p_x \text{Tr}((\dots)\log(\dots)) = \sum_x p_x S(\rho_x) + H(\{p_x\})$
代入后， $H(\{p_x\})$ 项被消掉，即可得到 $\chi$ 的表达式。

这个等式建立了编码（系综）和通信（互信息）之间的桥梁，是 Holevo 界 $I(X;Y) \le \chi$ 的基础。它表明，从一个系综中可提取的经典信息量，本质上等于该系综在经典-量子表示下的互信息。



### 经典信息与熵公式汇总

- **信息量**
  $$
  i(x)=-\log p_x
  $$
- **信息熵**
  $$
  H(X)=-\sum p_x\log p_x
  $$
- **条件熵**
  $$
  H(X|Y)=-\sum p_{xy}\log \frac{p_{xy}}{p_y}
  $$
  $$ H(X|Y)=H(X,Y)-H(Y)
  $$
  $$ H(X|Y)=\sum_y p_y H(X|Y=y)
  $$
- **互信息**
  $$
  I(X;Y)=H(X)-H(X|Y) = H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y)
  $$
- **相对熵**
  $$
  D ( p \| q ) \equiv \left\{ \begin{array} { l } \sum _ { x } p ( x ) \log ( p ( x ) / q ( x ) )~\text{if} ~\mathrm{sup}(p)\subset \mathrm{sup}(q) \\ + \infty ,\text{else}\end{array} \right.
  $$
- **条件互信息**
  $$
  I ( X ; Y | Z ) \equiv H ( Y | Z ) - H ( Y | X , Z )= H ( X | Z ) - H ( X | Y , Z )= H ( X | Z ) + H ( Y | Z ) - H ( X , Y | Z ) .
  $$
- **Rényi熵**
  $$
  H_{\alpha}(X)=\frac{1}{1-\alpha} \log \left(\sum_{i=1}^{n} p_{i}^{\alpha}\right)
  $$
- $\alpha=0$， Hartley熵或最大熵
- $\alpha \to 1$ ，Shannon熵
- $\alpha=2$， Collision熵、Rényi熵
- $\alpha \to\infty$， 最小熵
- **信道容量**：信道就是一个条件概率分布，当确定$X$的分布之后，根据条件概率就可以计算出联合分布，因此可以计算互信息，信道容量的定义要求我们遍历所有的$p_x$计算最大的互信息
  $$
  C=\max _{p_x} I(X ; Y)
  $$

### 量子信息与熵公式汇总

- **量子熵、冯诺依曼熵**
  $$
  S(\rho)=-\mathrm{tr}\left(\rho \log \rho\right)
  $$
- **条件熵**
  $$
  S ( A | B ) _ { \rho } \equiv S ( A B ) _ { \rho } - S ( B ) _ { \rho } .
  $$
- **相干信息量(coherent informaiton)**
  $$
  I\left(A\rangle B\right)_\rho =-S(A|B)_\rho .
  $$
- **量子互信息**
  $$
  I(A;B)=S(A)+S(B)-S(AB)
  $$
- **条件量子互信息**
  $$
  I(A;B|C)=S(A|C)+S(B|C)-S(AB|C)=S(AC)+S(BC)-S(C)-S(ABC)
  $$
- **量子相对熵**
  $$
  D ( p \| q ) \equiv \left\{ \begin{array} { l } \mathrm{tr}\left(\rho(\log \rho-\log\sigma) \right)~\text{if} ~\mathrm{sup}(\rho)\subset \mathrm{sup}(\sigma) \\ + \infty ,\text{else}\end{array} \right.
  $$
- **量子信道的互信息** (khatri2020PrinciplesQuantumCommunication, (7.11.102))
  $$
  I(\mathcal{E})=\max _{\rho \in \mathrm{D} (\mathcal{H}_ R\otimes \mathcal{H}_A )} I_{\mathrm{c}}(R;B )_{ (\mathcal{I}_R \otimes \mathcal{E} )(\rho)}
  $$
- **量子信道的Holevo信息量**
  $$
  \chi(\mathcal{N})=\max _{\left(\rho_x, p_x\right)_{x \in \mathsf{X}}} S\left(\sum_x p_x \mathcal{N}\left(\rho_x\right)\right)-\sum_r p_x S\left(\mathcal{N}\left(\rho_x\right)\right)
  $$
  在原始的定义中，最优化是关于所有的经典-量子态做的。
- **量子信道的经典容量** (khatri2020PrinciplesQuantumCommunication, Th.12.3)
  $$
  C(\mathcal{N})=\chi_{\mathrm{reg}}(\mathcal{N}):=\lim _{n \rightarrow \infty} \frac{1}{n} \chi\left(\mathcal{N}^{\otimes n}\right) .
  $$
- **量子信道的相干信息量** (chiribella2021IndefiniteCausalOrder, 4), (khatri2020PrinciplesQuantumCommunication, 7.11.107)
  $$
  I_{\mathrm{c}}(\mathcal{E}):=\max _{\rho \in \mathrm{D} (\mathcal{H}_A \otimes \mathcal{H}_A )} I_{\mathrm{c}}(A\rangle B )_{ (\mathcal{I}_A \otimes \mathcal{E} )(\rho)}
  $$
  $$ I_{\mathrm{c}}(\mathcal{E}):=\max _{\rho \in \mathrm{D} (\mathcal{H}_ R\otimes \mathcal{H}_A )} I_{\mathrm{c}}(R\rangle B )_{ (\mathcal{I}_R \otimes \mathcal{E} )(\rho)}
  $$
  上面的两个定义是否等价？
- **量子信道的量子容量** (khatri2020PrinciplesQuantumCommunication, 14.0.1)
  $$
  Q(\mathcal{N})=I_{\mathrm{reg}}^c(\mathcal{N}):=\lim _{n \rightarrow \infty} \frac{1}{n} I^c\left(\mathcal{N}^{\otimes n}\right)
  $$
- **[[量子信道的相对熵\|量子信道的相对熵]]**
  $$
  D(\mathcal{N} || \mathcal{M}) = \sup_{\rho_{RA}} D(\mathcal{N}_{A \to B}(\rho_{RA}) || \mathcal{M}_{A \to B}(\rho_{RA}))
  $$
- 更多公式，参见[[Barycentric bounds on the error exponents of quantum hypothesis exclusion#^vd2yiq\|Barycentric bounds on the error exponents of quantum hypothesis exclusion#^vd2yiq]]
---
**脚注**

[^1]: 终于有了令人愉悦的矩阵写法。
[^2]: 此式没有提供证明，但是经过程序验证，参见：`基本矩阵作为基底.nb`。
[^3]: 有一组基底拥有非常相似的性质：[Sylvester's generalized Pauli matrices](https://www.wikiwand.com/en/Generalizations_of_Pauli_matrices)。
[^4]: 参见：`几种量子信道quantum channel，信道-Kraus算符-蔡氏矩阵.nb`。
[^5]: Hilbert空间上的Wigner对称性操作都是幺正或者反幺正的 (Moretti2017, Th.12.11)。
[^6]: 4阶置换群的“置换形式”群元$(2,1,4,3)$表示被作用的列表中的第$1,2,3,4$个元素在置换后变为第$2,1,4,3$个元素，写成“循环形式”是$c(1,2)\circ c(3,4)$。利用PermutationCycles和PermutationList可以在置换形式和循环形式之间相互转换。
[^7]: `几种量子信道quantum channel，信道-Kraus算符-蔡氏矩阵.nb`。
[^8]: 参见函子范畴相关章节。