---
{"dg-publish":true,"permalink":"/1 Stories of Bizzo's Life/5 量子科学和量子技术/26 计算理论 计算物理/"}
---


## 可计算性
计算的本质是什么？早在电子计算机诞生之前的20世纪30年代，这个问题就已经有了完满的回答。很多种后来被证明为等价的关于计算的模型被相继提出， 其中包括递归函数、$\lambda$演算、图灵机等。

可计算性理论的出发点是来自数学、计算机科学和哲学领域的几个问题[[enderton2010compu\|enderton2010compu]]：如何对数学中的算法进行抽象和严格定义？（图灵机）如何根据算法的复杂性进行分类？（复杂性类）计算机程序能够完成的任务的限度是什么？（computable fucntion）有什么东西是超出限度的？（不可计算函数，停机问题等）什么是证明？（诉诸数理逻辑）真命题都能够被证明吗？（哥德尔不完备定理）

effectively calculable partial function是那些能够被有效地算出结果的函数，具体来说，她是一个吃进去长度为$k$的自然数数组$x$的函数$f$，如果数组$x$在定义域内，那么存在一个最终回停止的算法能够正确地返回$f$的值，如果数组$x$不在定义域内，那么永远不停止，并且不会返回任何结果。其中partial是指不是所有的$k$元数组都在$f$的定义域内。这个算法在数学上被抽象为图灵机，effectively calculable partial function被抽象为可计算函数。

> [!definition] 图灵机(TM)[Def.1.1
> - {Kitaev2002cqcomputation}]
>   一台**图灵机**的零件包括：
>
> - 有限集合$S$称为**字母表**(alphabet)，字母表中存在空格。
> - 子集$A\subset S$称为**外部字母表**(external)，$A$不包含空格。
> - 有限集合$Q$，其中的元素称为图灵机的**状态**。
> - 初始状态$q_0\in Q$。
> - 偏函数$\delta: Q \times S \rightarrow Q \times S \times\{-1,0,1\}$称为**转移函数**。

图灵机在一条被划分为小格子的无限长纸带上工作，每个格子有一个编号$p$，格子上写有字母表中的一个字母$s_p$。图灵机配备有一个指针指示当前聚焦的位置。在每一步操作中，图灵机根据它的指针的位置读取纸带上的字母$s_p$和自身的状态$q$和转移函数$\delta$计算$\delta\left(q, s_{p}\right)=\left(q^{\prime}, s^{\prime}, \Delta p\right)$，其中$\Delta p\in\{-1,0,1\}$，图灵机将位置$p$的字母改写为$s^\prime$，将自身状态改为$q^\prime$并将指针移动到$p+\Delta p$。对每一个步骤，可以定义图灵机的**配置**(configuration)，它一个三元组$\langle \sigma ; p;q\rangle$，其中$\sigma$是字母表$S$中的元素构成的一串字符，是当前纸带上的字符串，$p$是非负整数，表示当前指针的位置，$q$是图灵机的当前状态。图灵机的输入和输出是外部字母表$A$构成的字符串。从初始配置出发，图灵机一直运行直到$\delta$碰到没有定义的输入或者指针位置小于零，停止时，纸带上的外部字母表构成的字符串就是图灵机的输出。图灵机也可能永不停止，称输入字符串$\alpha$没有输出。

概率图灵机(PTM)有两个转移函数$\delta_{1,2}$，每一步都概率地使用两个转移函数中的其中一个。这一概念被用于定义NP问题和BPP问题。

记$A^*$为$A$能产生的所有字符串的集合，$A^*$的子集称为$A$的**语言**(language)。
> [!definition] 可计算函数
> - 从$A^*$到$A^*$的偏函数$f$称为**可计算偏函数**，如果任意取 $\alpha \in A^*$，存在图灵机$M$，使得$M$吃进去$\alpha$并输出结果，记为$\varphi_M(\alpha)$，满足$\varphi_M(\alpha)=f(\alpha)$。

**谓词(predicate)**$P$是一个从$A^*$到$\{0,1\}$的函数，如果$P$是可计算的，那么称谓词是**可判定的**。例如，我们可以问元素是否属于某个集合$V$，这就是一个谓词，如果这个谓词是可判定的，那么称集合$V$是**可判定的**。

丘奇的论文将一个非正式的想法与一个正式的想法联系起来：数学上有严格定义的computable partial function正确的建立了effectively calculable partial function的概念。它本身并不是一个能够被证明的数学陈述。但人们可以寻找支持或反对丘奇论点的证据，有利的证据越多，这个论题就越合理。目前为止还没有反例。另外，独立构建的计算模型，比如λ演算和图灵机等最终都被证明是等价的。就像函数的连续性的$\delta-\epsilon$定义一样，尽管可能和函数连续的直观理解在某种程度上有所出入，连续也已经被严格地数学化了，而且该定义非常有用。

在出现了概率性算法和量子计算以后，丘奇-图灵论题出现了一些变种[[wikiChurchTuring\|wikiChurchTuring]]。Ethan Bernstein 和 Umesh Vazirani提出了经典复杂性理论丘奇-图灵论题：概率图灵机可以高效地模拟任何现实的计算模型。量子复杂性理论的丘奇-图灵论题：量子图灵机可以高效地模拟任何现实的计算模型。如果最终人们证明了BPP严格小于BQP，也即量子计算在某些方面要比经典计算更加高效。那么经典复杂性理论丘奇-图灵论题将是错误的。当然，由于最原始的丘奇-图灵论题不涉及复杂性理论，只关注了函数的可计算性，所以其有效性不会受到影响。

## 计算复杂度
主条目:[[复杂性理论\|复杂性理论]]
关联：[[NP困难\|NP困难]] 


\begin{figure}
    \centering
    \begin{subfigure}{0.43\textwidth}
    \includegraphics[width=\textwidth]{image//2023-07-18-11-38-07.png}
    \caption{BQP}
    \label{fig:}
    \end{subfigure}
    \begin{subfigure}{0.43\textwidth}
    \includegraphics[width=\textwidth]{image//2023-07-18-11-40-00.png}
    \caption{RP}
    \label{fig:}
    \end{subfigure}
    \caption{两种计算复杂性类}
    \label{fig:}
\end{figure}

\begin{figure}[htbp]
 \centering
 \includegraphics[width=0.5\textwidth]{image//2023-07-18-11-47-14.png}
 \caption{ 复杂性类[[pinski2011adiabatic\|pinski2011adiabatic]]。未知：BPP与NP的关系；P与NP是否相等 }
 \label{  }
\end{figure}

## 并行计算
在学习并行计算之前，应该先学习如何尽可能地提高单核计算效率。
### 并行程序设计基础
并行计算机是指在同一时间内执行多条指令(或处理多个数据)的计算机。处理问题的步骤：
> [!abstract] Contents
> - 一个问题被分解成为一系列可以并行执行的离散部分；
> - 每个部分可以进一步被分解成为一系列离散指令；
> - 来自每个部分的指令可以在不同的处理器上被同时执行；
> - 需要一个总体的控制/协作机制来负责对不同部分的执行情况进行调度。

根据福林分类法，并行计算机分为
> [!abstract] Contents
> - Single instruction stream, single data stream (SISD)
> - Single instruction stream, multiple data streams (SIMD)
> - Multiple instruction streams, single data stream (MISD)
> - Multiple instruction streams, multiple data streams (MIMD)

根据储存方式分类，并行计算机可以分为：
> [!abstract] Contents
> - 共享内存并行计算机；
> - 分布式内存（具有局部处理单元）并行计算机；
> - 以及结合了前面两者的分布式共享内存并行计算机。

并行开销(parallel overhead)
> [!abstract] Contents
> - 任务启动时间；
> - 同步：在其它任务没有执行到这一同步点的时候，某一任务不能进一步执行后面的指令；
> - 数据通讯；
> - 由并行语言、链接库、操作系统等因素而导致的软件开销；
> - 任务终止时间。

 
根据进程之间的依赖关系并行算法可以分为同步并行算法、异步并行算法、纯并行算法。

**主从模式**的并行算法是指构成并行程序的进程中有一个主进程(master)，其余为从进程(slave)。主进程与从进程运行不同的代码，但所有从进程运行的代码是相同的。在这种模式中，主进程一般负责整个并行程序的控制，分配数据和计算任务给从进程，而从进程负责完成计算。

### 基本的MPI并行程序设计

MPI(Message Passing Interface)是一种并行消息传递库(规范/标准/协议，不是语言)。该标准提出了一种基于消息传递的函数接口描述。MPI本身并不是一个具体的实现，而只是一种标准描述。主要用于分布式和分布式共享内存并行机上多进程编程模型，支持FORTRAN，C，C++。

### OpenMP
OpenMP用于共享内存并行系统的多线程程序设计的一套指导性的编译处理方案(CompilerDirective)。由于使用线程间共享内存的方式协调并行计算，它在多核/多CPU结构上的效率很高、内存开销小、编程语句简洁直观，因此编程容易、编译器实现也容易。最大的缺点是只能在单台(多核)主机上工作，不能用于多台主机间的并行计算。支持FORTRAN，C，C++。

### Mathematica中的并行计算
> [!abstract] Contents
> - [Parallel Computing Tools User Guide](https://reference.wolfram.com/language/ParallelTools/tutorial/Overview.html)
> - [VIDEO COURSE: Parallel Computing in the Wolfram Language](https://www.wolfram.com/wolfram-u/parallel-computing/)

## 梯度下降法
梯度下降法{（[[邱锡鹏2020神经网络与深度学习\|邱锡鹏2020神经网络与深度学习]] Apd.C.2.2）} ( Gradient Descent Method), 也叫作最速下降法 ( Steepest Descend Method ), 经常用来求解无约束优化的最小值问题.
对于函数 $f(\boldsymbol{x})$, 如果 $f(\boldsymbol{x})$ 在点 $\boldsymbol{x}_{t}$ 附近是连续可微的, 那么 $f(\boldsymbol{x})$ 下降最快的 方向是 $f(x)$ 在 $\boldsymbol{x}_{t}$ 点的梯度方向的反方向.
根据泰勒一阶展开公式, 有
$$
f\left(\boldsymbol{x}_{t+1}\right)=f\left(\boldsymbol{x}_{t}+\Delta \boldsymbol{x}\right) \approx f\left(\boldsymbol{x}_{t}\right)+\Delta \boldsymbol{x}^{\top} \nabla f\left(\boldsymbol{x}_{t}\right) .
$$
要使得 $f\left(\boldsymbol{x}_{t+1}\right)<f\left(\boldsymbol{x}_{t}\right)$, 就得使 $\Delta \boldsymbol{x}^{\top} \nabla f\left(\boldsymbol{x}_{t}\right)<0$. 我们取 $\Delta \boldsymbol{x}=-\alpha \nabla f\left(\boldsymbol{x}_{t}\right)$. 如果 学习率$\alpha>0$ 为一个够小数值时, 那么 $f\left(\boldsymbol{x}_{t+1}\right)<f\left(\boldsymbol{x}_{t}\right)$ 成立.
这样我们就可以从一个初始值 $\boldsymbol{x}_{0}$ 出发, 通过迭代公式
$$
\boldsymbol{x}_{t+1}=\boldsymbol{x}_{t}-\alpha_{t} \nabla f\left(\boldsymbol{x}_{t}\right), t \geq 0 .
$$
生成序列 $\boldsymbol{x}_{0}, \boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots$ 使得
$$
f\left(\boldsymbol{x}_{0}\right) \geq f\left(\boldsymbol{x}_{1}\right) \geq f\left(\boldsymbol{x}_{2}\right) \geq \cdots
$$
如果顺利的话, 序列 $\left(\boldsymbol{x}_{n}\right)$ 收玫到局部最小解 $\boldsymbol{x}^{*}$. 注意, 每次迭代步长 $\alpha$ 可以 改变, 但其取值必须合适, 如果过大就不会收玫,如果过小则收玫速度太慢.\par 
线性方程组的$Ax=b$的解即$\varphi(x)=\frac{1}{2} \vec{x}^{T} A \vec{x}-\vec{b}^{T} \vec{x}$的全局最小值点，针对这一优化问题有改进的梯度下降法，即共轭梯度法，每次的下降方向与上一次的方向关于$A$共轭。

## 随机算法
整理自[[gobet2016monte\|gobet2016monte]]。蒙特卡洛方法是依靠随机抽样来获得问题的数值解的计算技术。这些方法在处理复杂系统或涉及不确定性的问题时特别有用。通过模拟随机事件，蒙特卡罗方法提供了所需数量或解决方案的近似值。另一方面，随机过程是描述受随机影响的系统演化的数学模型。它们用于模拟物理、金融、生物学和工程等领域的各种现象。随机过程对于理解和分析具有内在随机性的系统至关重要。\footnote{如何理解随机算法在某些问题上比确定性算法更优越？[[1 Stories of Bizzo's Life/5 量子科学和量子技术/27 量子算法\|量子算法]]引入随机性如何？}

### 随机数的生成与随机性检验
随机性检验包括均匀性检验和独立性检验。从正态分布总体中抽样出的样本方差服从卡方分布。

### 数值积分
为了计算积分
$$
I=\int_{[0,1]^{d}} f(x) \mathrm{d} x
$$
生成$M$个$d$维的随机向量$(U_1,\ldots  ,U_M)$，每个随机向量在$[0,1]^d$上服从均匀分布，蒙特卡洛法将以下随机变量的期望作为积分的估计值\footnote{不是统计推断中的估计值}
$$
I_{M}:=\frac{1}{M} \sum_{m=1}^{M} f\left(U_{m}\right)
$$
大数定理给出下式以概率1成立
$$
\lim \limits_{M \rightarrow+\infty} I_{M}=\mathbb{E}\left(f\left(U_{1}\right)\right)=\int_{[0,1]^{d}} f(x) \mathrm{d} x
$$
中心极限定理给出$I_M$依分布收敛\footnote{神秘的收敛方式}到正态分布
$$
\sqrt{M}\left(I_{M}-I\right) \underset{M \rightarrow+\infty}{\Longrightarrow} \mathcal{N}\left(0, \sigma^{2}\right)
$$
其中
$$
\sigma^{2}=\mathbb{E}\left(f^{2}\left(U_{1}\right)\right)-\left(\mathbb{E}\left(f\left(U_{1}\right)\right)\right)^{2}
$$
也可以用相同的采样方法来估计。由于收敛速度与维度$d$无关，当维度较大时，该方法的收敛速度往往快于确定性的方法。\par 
重要性采样能减少$\sigma$从而加速收敛。其主要思想是，为了估计积分$I$的值，不是抽取样本$I_M$，而是从另一个随机变量$P(x)$中抽样，并以$F/P$的期望作为积分结果的估计值。设$P$的概率密度函数为$p(x)$，将$I$重写为
$$
I=\int_{[0,1]^{d}} \frac{f(x)}{p(x)} p(x)\mathrm{d} x=\int_{} \frac{f(x)}{p(x)}\mathrm{d} P(x)\approx\left\langle\frac{f}{p}\right\rangle \pm \sqrt{\frac{\left\langle f^{2} / p^{2}\right\rangle-\langle f / p\rangle^{2}}{M-1}}
$$
其方差取决于$f(x)/p(x)$的方差，因此重要性抽样的目标就是减小该方差。

### 采样方法
  对于概率分布简单的随机变量$X$，常用反函数法进行采样，设$X$的累积分布函数为$F(x)$，$\xi$在$[0,1]$上服从均匀分布。抽取样本$\xi_i$，那么$F^{-1}(\xi_i)$就是$X$的样本。

对于较复杂的概率分布，可以用变换抽样法，将简单分布的样本变换为目标分布的样本。

在正则系综中，微观状态$x$出现的概率由该状态的能量$E$和系统的温度$T$确定
$$
P(x)=e^{\frac{F-E(x)}{k T}}
$$
由于在实际系统中$x$的样本空间$\mathcal{X}$很大，计算正则配分函数$Z=e^{-F /(k T)}$通常是不可能的。对$P(x)$进行采样通常使用取舍法(Rejection sampling)和Metropolis–Hastings算法。\par 
      下面给出取舍法的具体实现步骤。设随机变量$X$和$Y$的pdf分别是$f_X$和$f_Y$，且都定义在区间$[0,1]$上，取舍法可以从$Y$的样本中获得$X$的样本。设$u$是在区间$[0,1]$上均匀分布的样本，$y$是$Y$的样本\footnote{如果$y$是用CDF反函数法从$u$得到的，那么$X$将是和$Y$相同的分布，该方法不能得到正确的分布，详见取舍法.nb}，如果
$$
u<\frac{f_X(y)}{M f_Y(y)} =\frac{\text{target pdf}}{\text{large number}\times \text{aux pdf}}
$$
则接受$y$作为$X$的样本$x$，否则拒绝该值，重新抽样。其中$M>\mathrm{max}_{y\in \mathcal{X}}\frac{f_X(y)}{f_Y(y)}$，当$M$不满足该要求时，不能得到正确的抽样结果，而当$M$过大时，拒绝率很高，抽样效率也会降低。


 \includegraphics[width=0.5\textwidth]{image//2023-06-20-11-29-39.png}


Metropolis–Hastings算法将采样目标pdf$P(x)$视为某个马尔科夫链的stationary distribution，选取一个合适的初值$x_0$，在每步迭代中进行下列操作：
     
> [!abstract] Contents
> - 根据以$x_t$为中心的概率密度函数$g_{x_t}(x')$随机生成一个候选项$x'$，在对称提议中，要求$g_{x}(y)=g_{y}(x)$，通常取为高斯分布。
> - 计算接受比率$\alpha=P\left(x^{\prime}\right) / P\left(x_{t}\right)$
> - 生成在$[0,1]$上均匀分布的随机数$u$
> - 如果$u \leq \alpha$，则接受候选项，令$x_{t+1}=x'$，否则拒绝候选项，令$x_{t+1}=x_t$。

    
 

### 随机最优化

# 松散的内容

[GAP System for Computational Discrete Algebra ~ 计算离散代数的 GAP 系统 (gap-system.org)](https://www.gap-system.org/)
